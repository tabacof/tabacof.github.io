[
  {
    "objectID": "posts/how_not_to_forecast_election/index.html",
    "href": "posts/how_not_to_forecast_election/index.html",
    "title": "How (not) to forecast an election",
    "section": "",
    "text": "We use a hierarchical Bayesian model to show how a simple pro-Trump bias has a huge effect on forecasting election results. See the discussion on HackerNews.\n\nIntroduction\nUnlike other countries, where presidential elections follow a simple majority rule, the American election follows a different set of rules that makes it very hard for outsiders to understand what is going on. However, the idea is quite simple: each state has a number of electoral college votes, totaling 538 votes across the nation. The winner candidate of each state election takes all of its electoral college votes (with two exceptions). If a candidate receives 270 electoral votes or more overall, he or she wins the election. This makes the election an interesting forecasting problem. Instead of predicting the total number of votes in each candidate, the problem is forecasting the results of each state individually.\n\nPrediction is very difficult, especially if it’s about the future. (disputed)\n\nOur interest is not to forecast the election, which is not a random variable anymore (spoiler alert: Trump won). We want to show how a Bayesian forecasting model works, and propose a modification that explains why statistical models would fail so badly if there were polling biases.\n\n\nPolling\nIn theory, a poll should randomly sample the voting population. However, in practice there are several possible polling problems that can compromise their use in naive statistical analyses:\n\nShy Tory / Bradley / “shame” effects Voters are not comfortable with revealing their particular preference.\nSelection bias: People who answer the pollsters are from an unrepresentative subgroup (e.g. people who answer landline phones).\nStratification errors: Wrong subpopulation determination when doing stratified sampling.\nCandidate and (non)-voting correlation: As voting is not mandatory, the presidential preference of some people may be correlated with their chance of (not) voting.\nTemporal preference change: The preferences change over time (a lot of people make up their minds in the last week).\nSample noise: Any sample from a population will be noisy and its statistics will not be identical to the population statistics.\n\nSimple forecasts usually only consider the last item when estimating the margins of error or uncertainty. If you only consider this and nothing more, multiple polls will be treated as independent and unbiased, and the more polls you use, the smaller the forecasting errors will be, until there is almost certainty. The other effects are biases that will make the average of multiple polls unreliable and possibly useless. As they are not directly available for modeling, the only way to estimate them is by making strong assumptions or using polling results from previous elections. We do not attempt to model exactly those issues here, we rather include all of them in a bias term that is shared across all polls, and show that even a small bias term favorable to Trump completely changes the forecast.\n\n\nForecast\nWe leave the details of our hierarchical Bayesian forecasting model at the end, for the advanced reader. Now we show the results of its forecasts. However, depending on which polls we include, we have very different results. The polls to use in a forecasting model should be recent and of good quality (conducted with appropriate methodology and with past successes). As a proxy of quality, we use the grades from 538.\nWe found that the simple choice of which polls to use has a very large impact on the probability of victory of each candidate:\n\n\n\n\n\n\n\n\n\nPolling choice\nClinton\nTrump\nNeither\n\n\n\n\nLast poll with good grade\n87.3%\n10.5%\n2.2%\n\n\nPolls with the best 3 grades (over the last month)\n99.3%\n0.6%\n0.1%\n\n\nAll polls from last week\n100.0%\n0.0%\n0.0%\n\n\n\nWe consider polls with good grades to be above B, but this is not possible for some states, so we use a more tolerant definition in those cases. When we say last week or last month, we mean from the Election Day.\nBy aggregating the polls with different weights (e.g. according to their quality or distance from election), we would have endless forecasting possibilities, which explains the diversity found in the media before the election:\n\nNew York Times: 85% Clinton, 15% Trump\nHuffington Post: 98% Clinton, 1.7% Trump\nPrinceton Election Consortium: 99% Clinton\n538: 71.4% Clinton, 28.6% Trump\n\nOf the mainstream forecasters, only 538 cannot be included in the same bucket of certain Clinton victory. Notice that the other forecasts are consistent with our own results shown in the previous table. How come the forecasters, including us, made such egregious mistakes? As in many statistical problems, the answer lies with the data: garbage in, garbage out.\n\n\nBias impact\nWe encompass all possible polling issues in a general bias term. We use a bias term that is favorable to Trump on election day because that is clearly what happened on November 8. This can be interpreted as a hidden preference for Trump that is not captured by the polls by all the issues explained before. Instead of fixing the bias to an arbitrary value, we use a uniform random variable. We start with zero bias, where Clinton is almost surely the victor, and increase its span until Trump is almost certainly the victor:\n\n\n\nWe see that even a small polling bias has a huge effect on the election forecast. This explains why 538 had a more favorable Trump forecast as they included a polling bias and did not treat the polls as independent samples, but this also indicates that even 538 probably underestimated the polling biases.\nYou can check for yourself how the bias impacts the results of each state election on the map below. For each state we forecast the predicted percentage of votes of each candidate, with adjustable bias. Note that percentage of votes is different from the probability of winning in the state, which would also take the uncertainty of the prediction into account.\n\n\n\nHierarchical Bayesian forecasting model\nWe can use a hierarchical Bayesian model to aggregate the information of each state poll to form a globally coherent forecast. Overall, each poll’s likelihood is modeled as a multinomial, with Dirichlet prior (per state) and uniform hyperprior (per nation). This way, prior information is shared across states and we can use weakly informative hyperpriors. We start with an overall national preference over each candidate, modeled as three independent wide uniform distributions:\n\n\n\nThen, we have the voting intention in each state, with the national preference as prior:\n\n\n\nFinally, each poll is modeled as one independent sample of the voting intention of the state:\n\n\n\nWe infer the posteriors of the unknown parameters (state voting intentions and national preferences) given the observed variables (the polls we decided to include). The posterior is our knowledge of the unseen variables after observing statistically related variables. Depending on the choice of which polls to include as observations, as we explained before, the posteriors and thus the forecast will be different.\n\n\n\nTo forecast the probability of each candidate winning the state election, we use the same multinomial likelihood that was used for the inference. However, now the voting intentions are the posterior given the polls, and number of voters is chosen to match 2012 election numbers. Thus, for each state we sample the predicted number of votes of each candidate on election day using the following formula:\n\n\n\nThe candidate with more votes takes all the electoral colleges of the state (we ignore the particularities of Nebraska and Maine). We sum the electoral colleges votes of each candidates, and if a candidate wins 270 votes or more, he or she is the winner. We repeat this process multiple times in order to determine the probability of each candidate winning the election.\nTo add the bias term in our forecast to account for all the polling issues already cited, we make a simple change to the predictive model:\n\n\n\nThis bias always stochastically favors Trump. We must subtract the same value from Clinton in order to guarantee θbias remains a valid probability simplex. In our experiments above, we vary ϵ from 0 to 5%.\n\n\nStan Code\nTake a look at our code and feel free to play with it. Here is how we implemented our model in Stan:\ndata {\n    int nb_polls; // Number of polls\n    int nb_states; // Number of states (51 because of D.C.)\n    int nb_candidates; // Number of candidates (3: Trump, Clinton, Ind.)\n    int polls_states[nb_polls]; // Poll -> state map\n    int votes[nb_polls, nb_candidates]; // Polled votes for each candidate\n    int nb_voters[nb_states]; // Number of voters for forecasting\n    real bias; // Polling bias\n}\nparameters {\n    simplex[nb_candidates] theta[nb_states]; //1 - Trump, 2 - Clinton, 3 - Ind.\n    vector[nb_candidates] alpha;\n}\nmodel {\n    for(c in 1:nb_candidates)\n        alpha[c] ~ uniform(0, 1000); // Weakly informative hyperprior\n\n    for(s in 1:nb_states)\n        theta[s] ~ dirichlet(alpha); // Dirichlet prior per state\n\n    for(p in 1:nb_polls) // Multinomial observations (polled values)\n        votes[p] ~ multinomial(theta[polls_states[p]]);\n}\ngenerated quantities {\n    int votes_pred[nb_states, nb_candidates]; // Predicted number of votes on election day\n    real epsilon[nb_states]; // Bias random variable\n    simplex[nb_candidates] theta_bias[nb_states]; // Biased voting intentions\n\n    // The deltas below are used to ensure that the biased thetas form a valid simplex\n    real delta_t[nb_states];\n    real delta_h[nb_states];\n    real delta[nb_states];\n\n    for(s in 1:nb_states) {\n        if(bias == 0.0)\n            epsilon[s] <- 0.0;\n        else\n            epsilon[s] <- uniform_rng(0, bias); // Bias value for this state\n\n        // We must ensure that theta will remain a valid probability simplex,\n        // so we limit delta in a way theta will never be below 0 or above 1\n        delta_t[s] <- fabs(theta[s][1] - fmax(0.0, fmin(1.0, theta[s][1] + epsilon[s])));\n        delta_h[s] <- fabs(theta[s][2] - fmin(1.0, fmax(0.0, theta[s][2] - epsilon[s])));\n        delta[s] <- fmin(delta_t[s], delta_h[s]);\n\n        theta_bias[s][1] <- theta[s][1] + delta[s];\n        theta_bias[s][2] <- theta[s][2] - delta[s];\n        theta_bias[s][3] <- theta[s][3];\n\n        votes_pred[s] <- multinomial_rng(theta_bias[s], nb_voters[s]);\n    }\n}\n\n\nConclusion\nWe have found that even a modestly sophisticated statistical model does very little to counter unreliable data. A proper forecasting model for the American elections must include polling biases, as we have shown in a general way. To arrive at a precise number, you must either make assumptions on the polling methodology, or calibrate the polls weights using their historical reliability. This could be the reason that 538 had Trump winning at the highest probability of the mainstream media. We have also to consider that when you forecast 70% probability of winning, the prediction is expected to fail 30% of the time, so it is hard to evaluate models and forecasters using only one observation.\nEven though the 2016 election was one of most surprising polling misses in recent years, the result was not a black swan. Nassim Nicholas Taleb pointed out, before the election, that the mainstream forecasts were not reliable due to their significant volatility. According to his model based on option theory, this volatility should have pulled the probabilities toward 50-50. As a follow-up, if there is interest, we want to explore a time-series model where the voting intentions follow a random walk. To do this, we need to change the underlying model to allow the unseen random walk influence the polling results. Following Andrew Gelman’s suggestion, we can change the Dirichlet prior to a softmax prior, and then we can make the softmax parameters follow a random walk.\n\nAcknowledgements\nThis blog post was written with Ramon Oliveira back when we were working together in a data science consulting company (Datart) we co-founded."
  },
  {
    "objectID": "posts/neurips_2018/index.html",
    "href": "posts/neurips_2018/index.html",
    "title": "NeurIPS 2018: A Data Scientist’s Account",
    "section": "",
    "text": "Two weeks ago in Montreal, NeurIPS (formerly known as NIPS) took place, the world’s largest conference on machine learning and artificial intelligence. Major advancements in the field were presented, covering Deep Learning, GANs, and Reinforcement Learning, including both theory and practice. Over eight thousand people attended, more than a thousand papers were accepted, and dozens of workshops were held. Additionally, nearly all major tech companies were present, primarily aiming to recruit scientists and researchers. I went to NeurIPS to present the work Adversarial Attacks on Variational Autoencoders at the LatinX in AI workshop, co-authored by George Gondim, myself and Eduardo Valle.\nSpeaking of LatinX, this year diversity and inclusion were the hottest topics at the conference. We had three other workshops dedicated to this (Black in AI, Women in ML, Queer in AI), as well as smaller talks and gatherings on the subject. Despite this, more than half of the Black in AI participants couldn’t obtain visas in time or were denied, which had some repercussions and may influence future conference locations. At LatinX, the Latin American artificial intelligence meeting Khipu was announced, scheduled to take place in Montevideo in November 2019. We Brazilians cannot miss this opportunity!\nThe conference had its issues: tickets sold out in just 11 minutes; last-minute name change (and we were left without mugs because of it); overcrowded sessions, with people being kicked out due to “fire hazard”; numerous audiovisual problems (it would have been better to watch many talks from home). Despite this, the experience is unique; there’s no other place where you bump into Geoff Hinton, Yoshua Bengio, and Yann LeCun in the halls. Of course, they’ll always be surrounded by admirers and people wanting to take photos.\n\n\n\nImagine you’re explaining your poster and Geoff Hinton appears behind you?\n\n\nAlthough these “celebrities” were present, their students were the ones presenting the academic work. With over a thousand papers at the main conference and hundreds at the workshops, it’s hard to know everything that happened there. I noted down more than 30 articles to review more carefully later. Although this is a large amount to read, it’s only a fraction of the total. For those who want a taste of what’s published there, here are some examples:\nBest papers (according to reviewers and program committee):\n\nNeural Ordinary Differential Equations\nNon-delusional Q-learning and Value-iteration\nOptimal Algorithms for Non-Smooth Distributed Optimization in Networks\nNearly Tight Sample Complexity Bounds for Learning Mixtures of Gaussians via Sample Compression Schemes\n\nBest papers (according to me):\n\nRegularization Learning Networks: Deep Learning for Tabular Datasets\nBayesian Neural Network Ensembles\nThe Everlasting Database: Statistical Validity at a Fair Price\nHow to Start Training: The Effect of Initialization and Architecture\n\nBesides the academic side, NeurIPS has a strong social aspect: parties and gatherings. Excluding the official closing, the parties were sponsored by companies. I attended two (Nvidia and Element AI), with excellent food and drinks. There’s a bit of elitism; I only managed to attend these two parties thanks to a friend who works with big names in a Montreal lab. However, every day there were several options, with varying degrees of difficulty to get a ticket.\nIn my opinion, the gatherings organized by participants through the conference app were the best part of the week, as they brought together people with similar interests. I attended AI for Business and AI in Production, both highly relevant to my work as a data scientist at Nubank. There, it’s possible to meet people from various backgrounds and profiles who share the same interest, without the elitism of those exclusive parties.\n\n\n\nAI for Business lunch (I’m wearing purple in the top right photo)\n\n\nFinally, the last two days were dedicated to workshops. In them, you could delve deep into a specific topic and even learn about things not officially published anywhere yet. The workshop that left the greatest impression on me was AI in Financial Services. However, it had a strong bias toward North American and European realities, mostly discussing regulations and how machine learning could be done within these constraints. For example, in those countries, you need to explain the reasons for denying a loan, but how do you do that if the loan risk is calculated by a deep neural network? However, many countries like China, India, and Brazil don’t have such restrictions, so our challenges are different and were barely explored there. I can say that Nubank is at the forefront of applying machine learning to financial products globally.\nNeurIPS is an academic conference, not the place to meet other data scientists but rather to find machine learning and AI researchers. I recommend the experience for those with an academic inclination, who have the habit of reading papers and plan to, or have already published work in the field. If you have other interests, there are conferences and meetings that may be more useful professionally and more accessible, such as Strata, PAPIs.io or even KDD. The most important thing is to get out there, meet new people, have an enriching experience, but don’t forget to take some time to explore and enjoy the trip!"
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html",
    "href": "posts/hierarchy_needs_ml/index.html",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "",
    "text": "In 1943, Abraham Maslow created the hierarchy of human needs, ranging from basic physiological needs to abstract concepts like self-actualization. In this article, I propose a hierarchy of machine learning needs:\nA framework like this can be useful for answering questions like:\nI try to answer these questions and more at the end of this article, but first, it’s necessary to define and better understand each need in the hierarchy."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#the-hierarchy-of-needs",
    "href": "posts/hierarchy_needs_ml/index.html#the-hierarchy-of-needs",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "The hierarchy of needs",
    "text": "The hierarchy of needs\n\nBusiness\nBusiness sits at the base of the pyramid, as it’s the foundation everything is built upon. Directly or indirectly, data scientists must always strive to deliver value to the business. This way, we can impact customers positively, secure resources (whether human or computational), and advance in our careers. This pragmatic view may be discouraging for those seeking technical challenges only, but I propose that business challenges are more difficult and unique than those found in machine learning competitions.\nNo e-commerce company aims solely to predict customer churn; the real goal is to take actions that reduce churn among the most valuable customers. Credit risk assessment alone isn’t very useful; deciding who will receive credit and for how much is the core of many financial institutions. The probability of lead conversion is just the first step in prioritizing and allocating sales resources in a B2B company. In all three examples, the focus is on intervention (causality) rather than just prediction (machine learning).\nI worked on a document classification project where there initially seemed to be no complicated business issues involved; achieving sufficient accuracy would ensure the project’s success. After talking to the actual users, I realized that the classification rationale was more important than the decision itself. In the same place, a project had previously failed for considering only the model’s AUC without any concern for its practical use. We shifted our focus and delivered a system that was genuinely useful for the users and the company by investing more in the UI and less in the modelling.\n\n\nTarget\nThe target is what the model will try to predict, which may not be obvious at first:\n\nChurn: What should be the time window to define churn? What if the user becomes active again?\nCredit: How long must one go without paying to be a default? What if the collections team gets all the money back?\nSales lead: At what point in the sales funnel do we define conversion? What if there is a refund?\nDocument classification: What should be done with sub-categories? Can we group smaller categories into “others”?\n\nChoosing the target is the most critical step in modelling. No features or models can save an inappropriate target. On the other hand, having an appropriate target allows even very simple models (such as linear regression) with basic features to have some impact and already be deployed to production.\n\n\nEvaluation/Metrics\nEvaluation is the framework that objectively assesses how well a model will perform when deployed. The evaluation process should closely resemble what happens after the model’s deployment. While the standard holdout or cross-validation splits provide a starting point, they are generally insufficient. If the problem evolves over time (as is the case for nearly all business problems), a time-series split should be employed. If the target is censored for six months, there should be a six-month gap between training and testing. If the data contains groups and the model will make predictions for new groups in the future, you should use a group split.\nMetrics judge how well a model is predicting its target. It’s not uncommon to use more than one evaluation metric for the same problem. For example, in a binary classification problem, one might use AUC to assess how well the model ranks examples and log loss to evaluate whether the probabilities are well calibrated. It’s also common to consider business metrics, such as expected conversion rate or the number of credit approvals. These metrics are harder to estimate offline and generally require some assumptions or experiments. Although handling these metrics may be more difficult, they serve as a more powerful guide than traditional machine learning metrics. Plus, communicating results with people from other areas becomes much easier!\n\n\nFeatures\nFeatures are the inputs of the model. They need to be predictive of the target. It’s crucial to avoid leakage, meaning that when deploying the model in production, features must appear in the same way they did during training. I’ve encountered leakage in a conversion prediction project where I used features that only appeared when the user converted. In other cases, the values were missing. The model learned that missing values were never associated with conversions and achieved a 100% AUC. However, this model had no real value for the business!\nFeature engineering can be partially automated with tools like Featuretools. However, most of the work in creating new features depends on understanding the problem and the available data (including what can be crawled or purchased externally).\nAre more features always better? Not necessarily. More features may require more monitoring and engineering, which may not be a good trade-off in certain cases. It’s essential to balance the value of features (possibly with a business metric) with their operational and maintenance costs.\n\n\nModels\nIn the end, given the constraints of business, target, evaluation/metrics, and features, the choice of models narrows considerably. If the business requires interpretability of predictions, you shouldn’t use a neural network. If the target is continuous, you want a regression model, not a classification model. If the metric evaluates the calibration of probabilities, you want a model that can learn a proper scoring rule. If you have more features than examples, you want a model that can ignore most features, like Lasso regression.\nThe modelling process can be automated with tools like auto-sklearn or PyCaret, but only if it makes sense for the business. In some cases, gaining an additional percentage point in accuracy is less useful than having interpretable decisions, communicating with other areas about how the model works (including external regulators), training speed for “big data” cases, and prediction latency for real-time systems."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#applications",
    "href": "posts/hierarchy_needs_ml/index.html#applications",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "Applications",
    "text": "Applications\nWe can apply the hierarchy of needs to better understand the reality of machine learning, which is difficult to learn solely from MOOCs or competitions:\n\nThe Spiral of Applied Machine Learning\nIn practice, machine learning is not a linear process, starting with a business problem and ending with a model. The process is repeated several times, and each iteration feeds the next:\n\nThis suggests starting simple on the modelling side. Always define a baseline first, which can be a business rule of thumb or a simple model (e.g. a linear regression or a decision tree classifier). The first proper model you build to compare against the baseline should be pragmatic, which depends on your domain:\n\nTabular data: Use LightGBM (watch my PyData London presentation here) or XGBoost\nTime series forecasting: Use Auto-ARIMA or Prophet\nText classification: Use word counts and Naive Bayes, TF-IDF and logistic regression, or ChatGPT API\nImage recognition: Use ResNet with transfer learning or ChatGPT4 API\n\nThis will handle the “model” part of the spiral and probably capture most of the gains. Use your time to focus on the other components before coming back to the model: deploy it as soon as possible, get feedback from the stakeholders, explore the failure modes (error analysis), and then go over the target-evaluation-metrics-features-model process again.\n\n\nAuto ML\nMachine learning automation operates only at the highest levels of the pyramid: primarily in modelling, followed by feature engineering and selection. The other needs are much harder to automate and are the biggest differentiators among data scientists since they depend on domain knowledge. Even in the modelling part, there are cases where maximizing a single metric does not capture the whole story:\nOnce, while I was in the process of updating models, an analyst noticed that there was a small sub-population where the new model provided predictions that made no sense. In terms of metrics, there was no doubt that the new model was better, but for the business, it was not good to make mistakes in this sub-population. In this case, since I was using a relatively interpretable model, I managed to discover the reason for the incorrect predictions (related to how the missing values were being handled) and could deploy an appropriate solution.\n\n\nMonitoring\nMonitoring a model in production should be done at all levels of the hierarchy. Monitoring is generally associated with evaluation metrics, but other needs should not be ignored. For example, in a credit card fraud problem, the target is generally censored for several months (since it takes time to determine whether fraud has occurred or not, as it is a manual process), meaning that metrics can only be calculated months after each model decision. In this case, it is important to evaluate how the target is changing over time (using proxies with shorter censoring), monitor whether the model’s output distribution remains stable over time, and whether the distribution of features remains the same, which is a significant challenge in itself. For more information on model monitoring, I highly recommend watching Lina Weichbrodt’s PyData Berlin presentation.\n\n\nKaggle\nIn the case of Kaggle, the entire challenge lies in modelling, feature engineering and evaluation. The metric and target are already given. The business aspect is not explicitly present. With this, we can see the limitations of Kaggle as training and evaluation for a data scientist who will work on real-world problems.\nKaggle is a great tool for its purpose and is highly valued in the selection process for some positions. However, a data scientist needs to go beyond this and try to tackle other types of problems, those that are not well-formulated and therefore are fertile ground for exploring targets, metrics, and the use of predictive models in decision-making."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#conclusion",
    "href": "posts/hierarchy_needs_ml/index.html#conclusion",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, I tried to convey the perspective of a machine learning practitioner, undoubtedly biased by my own particular experience. However, I believe the points raised are relevant to many other data scientists, especially those coming from an academic background. I hope you can use the hierarchy of needs to better guide decisions made as practitioners or students in the field.\nThis blog post originally appeared in the Data Hackers Medium in 2019. Full disclosure: the translation was done with the help of GPT4. I reviewed the final text and updated some sections, since I’ve personally learned and grown a lot since 2019. I’m open to feedback or suggestions for more applications of the hierarchy of needs framework."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "The Hierarchy of Machine Learning Needs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nNeurIPS 2018: A Data Scientist’s Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2018\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nHow (not) to forecast an election\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2016\n\n\nPedro Tabacof\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Personal blog of Pedro Tabacof.\nReach out to me via Linkedin, Twitter, or last name at gmail dot com."
  }
]