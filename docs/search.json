[
  {
    "objectID": "posts/how_not_to_forecast_election/index.html",
    "href": "posts/how_not_to_forecast_election/index.html",
    "title": "How (not) to forecast an election",
    "section": "",
    "text": "We use a hierarchical Bayesian model to show how a simple pro-Trump bias has a huge effect on forecasting election results. See the discussion on HackerNews.\n\nUS presidential elections\nUnlike other countries, where presidential elections follow a simple majority rule, the American election follows a different set of rules that makes it very hard for outsiders to understand what is going on. However, the idea is quite simple: each state has a number of electoral college votes, totaling 538 votes across the nation. The winner candidate of each state election takes all of its electoral college votes (with two exceptions). If a candidate receives 270 electoral votes or more overall, he or she wins the election. This makes the election an interesting forecasting problem. Instead of predicting the total number of votes in each candidate, the problem is forecasting the results of each state individually.\nOur interest is not to forecast the 2016 election, which is not a random variable anymore (spoiler alert: Trump won). We want to show how a Bayesian forecasting model works, and propose a modification that explains why statistical models would fail so badly if there were polling biases.\n\n\nPolling\nIn theory, a poll should randomly sample the voting population. However, in practice there are several possible polling problems that can compromise their use in naive statistical analyses:\n\nShy Tory and Bradley effects: Voters are not comfortable with revealing their particular preference.\nSelection bias: People who answer the pollsters are from an unrepresentative subgroup (e.g. people who answer landline phones).\nStratification errors: Wrong subpopulation determination when doing stratified sampling.\nCandidate and (non)-voting correlation: As voting is not mandatory, the presidential preference of some people may be correlated with their chance of (not) voting.\nTemporal preference change: The preferences change over time (a lot of people make up their minds in the last week).\nSample noise: Any sample from a population will be noisy and its statistics will not be identical to the population statistics.\n\nSimple forecasts usually only consider the last item when estimating the margins of error or uncertainty. If you only consider this and nothing more, multiple polls will be treated as independent and unbiased, and the more polls you use, the smaller the forecasting errors will be, until there is almost certainty. The other effects are biases that will make the average of multiple polls unreliable and possibly useless. As they are not directly available for modeling, the only way to estimate them is by making strong assumptions or using polling results from previous elections. We do not attempt to model exactly those issues here, we rather include all of them in a bias term that is shared across all polls, and show that even a small bias term favorable to Trump completely changes the forecast.\n\n\nForecast\nWe leave the details of our hierarchical Bayesian forecasting model at the end, for the advanced reader. Now we show the results of its forecasts. However, depending on which polls we include, we have very different results. The polls to use in a forecasting model should be recent and of good quality (conducted with appropriate methodology and with past successes). As a proxy of quality, we use the grades from 538.\nWe found that the simple choice of which polls to use has a very large impact on the probability of victory of each candidate:\n\n\n\n\n\n\n\n\n\nPolling choice\nClinton\nTrump\nNeither\n\n\n\n\nLast poll with good grade\n87.3%\n10.5%\n2.2%\n\n\nPolls with the best 3 grades (over the last month)\n99.3%\n0.6%\n0.1%\n\n\nAll polls from last week\n100.0%\n0.0%\n0.0%\n\n\n\nWe consider polls with good grades to be above B, but this is not possible for some states, so we use a more tolerant definition in those cases. When we say last week or last month, we mean from the Election Day (Nov 8, 2016).\nBy aggregating the polls with different weights (e.g. according to their quality or distance from election), we would have endless forecasting possibilities, which explains the diversity found in the media before the election:\n\nNew York Times: 85% Clinton, 15% Trump\nHuffington Post: 98% Clinton, 1.7% Trump\nPrinceton Election Consortium: 99% Clinton\n538: 71.4% Clinton, 28.6% Trump\n\nOf the mainstream forecasters, only 538 cannot be included in the bucket of certain Clinton victory. Notice that the other forecasts are consistent with our own results shown in the previous table. How come the forecasters, including us, made such egregious mistakes? As in many statistical problems, the answer lies with the data: garbage in, garbage out.\n\n\nBias impact\nWe encompass all possible polling issues in a general bias term. We use a bias term that is favorable to Trump on election day because that is clearly what happened on November 8. This can be interpreted as a hidden preference for Trump that is not captured by the polls by all the issues explained before. Instead of fixing the bias to an arbitrary value, we use a uniform random variable. We start with zero bias, where Clinton is almost surely the victor, and increase its span until Trump is almost certainly the victor:\n\n\n\n\n\nWe see that even a small polling bias has a huge effect on the election forecast. This explains why 538 had a more favorable Trump forecast as they included a polling bias and did not treat the polls as independent samples, but this also indicates that even 538 probably underestimated the polling biases.\nYou can check for yourself how the bias impacts the results of each state election on the map below. For each state we forecast the predicted percentage of votes of each candidate, with varying bias:\n\n\n\nHierarchical Bayesian forecasting model\nWe can use a hierarchical Bayesian model to aggregate the information of each state poll to form a globally coherent forecast. Overall, each poll’s likelihood is modeled as a multinomial, with Dirichlet prior (per state) and uniform hyperprior (per nation). This way, prior information is shared across states and we can use weakly informative hyperpriors. We start with an overall national preference over each candidate, modeled as three independent wide uniform distributions:\n\n\n\nThen, we have the voting intention in each state, with the national preference as prior:\n\n\n\nFinally, each poll is modeled as one independent sample of the voting intention of the state:\n\n\n\nWe infer the posteriors of the unknown parameters (state voting intentions and national preferences) given the observed variables (the polls we decided to include). The posterior is our knowledge of the unseen variables after observing statistically related variables. Depending on the choice of which polls to include as observations, as we explained before, the posteriors and thus the forecast will be different.\n\n\n\nTo forecast the probability of each candidate winning the state election, we use the same multinomial likelihood that was used for the inference. However, now the voting intentions are the posterior given the polls, and number of voters is chosen to match 2012 election numbers. Thus, for each state we sample the predicted number of votes of each candidate on election day using the following formula:\n\n\n\nThe candidate with more votes takes all the electoral colleges of the state (we ignore the particularities of Nebraska and Maine). We sum the electoral colleges votes of each candidates, and if a candidate wins 270 votes or more, he or she is the winner. We repeat this process multiple times in order to determine the probability of each candidate winning the election.\nTo add the bias term in our forecast to account for all the polling issues already cited, we make a simple change to the predictive model:\n\n\n\nThis bias always stochastically favors Trump. We must subtract the same value from Clinton in order to guarantee θbias remains a valid probability simplex. In our experiments above, we vary ϵ from 0 to 5%.\n\n\nStan Code\nTake a look at our code and feel free to play with it. Here is how we implemented our model in Stan:\ndata {\n    int nb_polls; // Number of polls\n    int nb_states; // Number of states (51 because of D.C.)\n    int nb_candidates; // Number of candidates (3: Trump, Clinton, Ind.)\n    int polls_states[nb_polls]; // Poll -> state map\n    int votes[nb_polls, nb_candidates]; // Polled votes for each candidate\n    int nb_voters[nb_states]; // Number of voters for forecasting\n    real bias; // Polling bias\n}\nparameters {\n    simplex[nb_candidates] theta[nb_states]; //1 - Trump, 2 - Clinton, 3 - Ind.\n    vector[nb_candidates] alpha;\n}\nmodel {\n    for(c in 1:nb_candidates)\n        alpha[c] ~ uniform(0, 1000); // Weakly informative hyperprior\n\n    for(s in 1:nb_states)\n        theta[s] ~ dirichlet(alpha); // Dirichlet prior per state\n\n    for(p in 1:nb_polls) // Multinomial observations (polled values)\n        votes[p] ~ multinomial(theta[polls_states[p]]);\n}\ngenerated quantities {\n    int votes_pred[nb_states, nb_candidates]; // Predicted number of votes on election day\n    real epsilon[nb_states]; // Bias random variable\n    simplex[nb_candidates] theta_bias[nb_states]; // Biased voting intentions\n\n    // The deltas below are used to ensure that the biased thetas form a valid simplex\n    real delta_t[nb_states];\n    real delta_h[nb_states];\n    real delta[nb_states];\n\n    for(s in 1:nb_states) {\n        if(bias == 0.0)\n            epsilon[s] <- 0.0;\n        else\n            epsilon[s] <- uniform_rng(0, bias); // Bias value for this state\n\n        // We must ensure that theta will remain a valid probability simplex,\n        // so we limit delta in a way theta will never be below 0 or above 1\n        delta_t[s] <- fabs(theta[s][1] - fmax(0.0, fmin(1.0, theta[s][1] + epsilon[s])));\n        delta_h[s] <- fabs(theta[s][2] - fmin(1.0, fmax(0.0, theta[s][2] - epsilon[s])));\n        delta[s] <- fmin(delta_t[s], delta_h[s]);\n\n        theta_bias[s][1] <- theta[s][1] + delta[s];\n        theta_bias[s][2] <- theta[s][2] - delta[s];\n        theta_bias[s][3] <- theta[s][3];\n\n        votes_pred[s] <- multinomial_rng(theta_bias[s], nb_voters[s]);\n    }\n}\n\n\nConclusion\nWe have found that even a modestly sophisticated statistical model does very little to counter unreliable data. A proper forecasting model for the American elections must include polling biases, as we have shown in a general way. To arrive at a precise number, you must either make assumptions on the polling methodology, or calibrate the polls weights using their historical reliability. This could be the reason that 538 had Trump winning at the highest probability of the mainstream media. We have also to consider that when you forecast 70% probability of winning, the prediction is expected to fail 30% of the time, so it is hard to evaluate models and forecasters using only one observation.\nEven though the 2016 election was one of most surprising polling misses in recent years, the result was not a black swan. Nassim Nicholas Taleb pointed out, before the election, that the mainstream forecasts were not reliable due to their significant volatility. According to his model based on option theory, this volatility should have pulled the probabilities toward 50-50. As a follow-up, if there is interest, we want to explore a time-series model where the voting intentions follow a random walk. To do this, we need to change the underlying model to allow the unseen random walk influence the polling results. Following Andrew Gelman’s suggestion, we can change the Dirichlet prior to a softmax prior, and then we can make the softmax parameters follow a random walk.\n\nAcknowledgements\nThis blog post was written with the help of Ramon Oliveira back when we were working in a data science consulting company (Datart) that we co-founded together."
  },
  {
    "objectID": "posts/name_classification/name_classification.html",
    "href": "posts/name_classification/name_classification.html",
    "title": "Name classification with ChatGPT",
    "section": "",
    "text": "I explore the problem of name classification with ChatGPT and three machine learning models of increasing complexity: from logistic regression to FastAI LSTM to Hugging Face transformer. To see all the code and reproduce the results, check out the notebook."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#name-classification",
    "href": "posts/name_classification/name_classification.html#name-classification",
    "title": "Name classification with ChatGPT",
    "section": "Name classification",
    "text": "Name classification\nCan you classify a name as belonging to a person or company? Some are easy, like “Google” is a company and “Pedro Tabacof” is a name. Some are trickier, like “John Deere”. With a labelled dataset, we can train a machine learning model to classify names into entities. This is a simplification of the more general task called Named Entity Recognition. This can also be seen as a simple version of document classification, where the document is simply a name. Due to its simplicity and relation to typical NLP problems, name classification is a good candidate to experiment with different NLP technologies.\nWhen I heard a friend was working on a name classification problem as part of a hiring process, I went straight to ChatGPT to look for answers. I soon realised that ChatGPT can do a great job itself classifying names into entities with just a couple of examples (one-shot learning):\n\nNow, if I actually productionize that prompt using ChatGPT’s API, how would it compare to more traditional alternatives? In NLP, traditional might mean a model from just 5 years ago!\nIn this post, I explore four ways to classify names into person or company:\n\nBaseline using word counts and logistic regression: typical baseline for text classification\nFastAI LSTM fine-tuning (whole network): simple fine-tuning with few lines of code\nHuggingface DistilBERT fine-tuning (head only): more involved neural network training using PyTorch\nChatGPT API one-shot learning: only prompt engineering and post-processing are needed\n\nI use two public datasets available on Kaggle: IMDb Dataset for people names and 7+ Million Company Dataset for companies. Those datasets are large, with almost 20 million names! The choice of datasets was inspired by the open-source business individual classifier by Matthew Jones, which achieves 95% accuracy on this name classification task.\nFor simplicity, I sample 1M names for training and 100k for testing with a 50-50 balance between companies and people. Since we have balanced classes and ChatGPT cannot produce scores or probabilities (so we cannot use ROC AUC or average precision, definitely a big limitation of ChatGPT), I decided to use the accuracy as the main metric."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#datasets",
    "href": "posts/name_classification/name_classification.html#datasets",
    "title": "Name classification with ChatGPT",
    "section": "Datasets",
    "text": "Datasets\nFirst, I will download the datasets from Kaggle and do some basic preprocessing. To reproduce the results, you will need a Kaggle account and its command line installed locally. You need to add your API key and username to a file kaggle.json, which is found in the directory defined by the environment variable called KAGGLE_CONFIG_DIR.\n\n\nDownloading free-7-million-company-dataset.zip to /notebooks\n 95%|██████████████████████████████████████  | 265M/278M [00:03<00:00, 93.1MB/s]\n100%|████████████████████████████████████████| 278M/278M [00:03<00:00, 80.1MB/s]\n\n\n\n\nDownloading imdb-dataset.zip to /notebooks\n 99%|██████████████████████████████████████▋| 1.05G/1.06G [00:06<00:00, 153MB/s]\n100%|███████████████████████████████████████| 1.06G/1.06G [00:06<00:00, 163MB/s]\n\n\n\n\nArchive:  free-7-million-company-dataset.zip\n  inflating: companies_sorted.csv    \n\n\n\n\nArchive:  imdb-dataset.zip\n  inflating: data.tsv                \n\n\nI do some preprocessing, inspired by the open-source repo I got the datasets inspiration from: 1. Lower case the people dataset since the companies dataset is all lower case (otherwise I’d suggest keeping the original case, as that can be informative). 2. Remove odd characters and unnecessary spaces. 3. Remove empty and null rows.\n\ncompanies = pd.read_csv(\"companies_sorted.csv\", usecols=[\"name\"])\n\npeople = (\n    pd.read_csv(\"data.tsv\", sep=\"\\t\", usecols=[\"primaryName\"])\n    # Since the companies are all lower case, we do the same here to be fair\n    .assign(name=lambda df: df.primaryName.str.lower()).drop(\"primaryName\", axis=1)\n)\n\ndf = pd.concat(\n    (companies.assign(label=\"company\"), people.assign(label=\"person\"))\n).sample(frac=1.0, random_state=42)\n\ninvalid_letters_pattern = r\"\"\"[^a-z0-9\\s\\'\\-\\.\\&]\"\"\"\nmultiple_spaces_pattern = r\"\"\"\\s+\"\"\"\n\ndf[\"clean_name\"] = (\n    df.name.str.lower()\n    .str.replace(invalid_letters_pattern, \" \", regex=True)\n    .str.replace(multiple_spaces_pattern, \" \", regex=True)\n    .str.strip()\n)\n\ndf = df[\n    ~df.clean_name.isin([\"\", \"nan\", \"null\"]) & ~df.clean_name.isna() & ~df.label.isna()\n][[\"clean_name\", \"label\"]]\n\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      name\n      label\n    \n  \n  \n    \n      10103038\n      jinjin wang\n      person\n    \n    \n      5566324\n      native waterscapes, inc.\n      company\n    \n    \n      8387911\n      jeff killian\n      person\n    \n    \n      6783284\n      lisa mareck\n      person\n    \n    \n      9824680\n      pablo sánchez\n      person\n    \n    \n      6051614\n      dvc sales\n      company\n    \n    \n      6479728\n      orso balla\n      person\n    \n    \n      4014268\n      two by three media\n      company\n    \n    \n      2093936\n      house of light and design\n      company\n    \n    \n      11914237\n      hamdy faried\n      person\n    \n  \n\n\n\n\nFrom the value counts below, we can see that we have 19.5 million names, 63% being people and 37% companies.\n\ndf.label.value_counts()\n\nperson     12344506\ncompany     7173422\nName: label, dtype: int64\n\n\nI sample 550k people and companies to make the dataset balanced and then split into 1M training and 100k testing examples.\n\ntrain_df = pd.concat(\n    (\n        df[df.label == \"company\"].sample(n=1_100_000 // 2),\n        df[df.label == \"person\"].sample(n=1_100_000 // 2),\n    )\n)\n\ntrain_df, test_df = train_test_split(train_df, test_size=100_000, random_state=42)\n\nI save the processed datasets for easier iteration. Tip: If you have large datasets, always try to save your preprocessed datasets to disk to prevent wasted computation.\n\n# Saving the processed dataframes locally for quicker iterations\ntrain_df.to_csv(\"train_df.csv\", index=False)\ntest_df.to_csv(\"test_df.csv\", index=False)\n\n# Freeing up the memory used by the dataframes\ndel companies, people, df, train_df, test_df\ngc.collect()\n\nSince I freed up the memory of all datasets, I need to reload them:\n\n# Just run from here if the datasets already exist locally\ntrain_df = pd.read_csv(\"train_df.csv\")\ntest_df = pd.read_csv(\"test_df.csv\")\n\ntrain_df.shape, test_df.shape\n\n((1000000, 2), (100000, 2))\n\n\nNow, I have one single dataset for training with 500k people and 500k companies and one single test set with 50k people and 50k companies."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#exploratory-data-analysis",
    "href": "posts/name_classification/name_classification.html#exploratory-data-analysis",
    "title": "Name classification with ChatGPT",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nBefore I actually get to the fun part, let’s understand the data we have first. I have two hypotheses to explore:\n\nDo we see a different distribution of words per class? I’d expect some words like “ltd” to be present only in companies and words like “john” to be over-represented in names.\nDoes sentence length vary by class? I expect higher range for companies than people, as companies can be from just two characters like “EY” to mouthfuls like “National Railroad Passenger Corporation, Amtrak”. Alternatively, I could look at the number of words per sentence, since most Western names are around 3 words.\n\nAnyway, beware the Falsehoods Programmers Believe About Names.\n\nwords_df = (\n    train_df.assign(word=train_df.clean_name.str.split(\" +\"))\n    .explode(\"word\")\n    .groupby([\"word\", \"label\"])\n    .agg(count=(\"clean_name\", \"count\"))\n    .reset_index()\n)\n\ntotal_words = words_df[\"count\"].sum()\n\nwords_df = words_df.assign(freq=words_df.count/total_words)\n\nperson_words = (\n    words_df[words_df.label == \"person\"].sort_values(\"freq\", ascending=False).head(25)\n)\ncompany_words = (\n    words_df[words_df.label == \"company\"].sort_values(\"freq\", ascending=False).head(25)\n)\n\nFirst, let’s take a look at the word counts by label:\n\n\n\n                                                \n\n\n\n\n\n                                                \n\n\nWe can see our hypothesis was right: Some words are quite predictive of being a person or company name. Note that there is no intersection between the top 25 words for people and companies. This insight implies a simple but effective baseline would be a model built on top of word count, which is what I do next. However, there is a long tail of possible names, so we have to go beyond the most common ones. Another way to see how the distributions differ is by sentence length:\n\n\n\n                                                \n\n\nCompany names tend to be longer on average and have a higher variance, but interestingly they both peak at 13 characters. I could use sentence length as a feature, but let’s stick to word counts for now."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#baseline-word-counts-logistic-regression",
    "href": "posts/name_classification/name_classification.html#baseline-word-counts-logistic-regression",
    "title": "Name classification with ChatGPT",
    "section": "Baseline: Word counts + Logistic regression",
    "text": "Baseline: Word counts + Logistic regression\nLet’s start with a simple and traditional NLP baseline: word frequency and logistic regression. Alternatively, we could use Naive Bayes, but I prefer logistic regression for its greater generality and easier interpretation as a linear model.\nTypically, we use TF-IDF instead of word counting for document classification. Since names are quite short and repetitive words (e.g. “John”) are predictive, I believe it not to be useful here. Indeed, a quick test showed no improvement to accuracy by using TF-IDF.\nAnother varation is to use n-grams for either words or characters: they’re left as a suggestion to the reader.\n\ntext_transformer = CountVectorizer(analyzer=\"word\", max_features=10000)\nX_train = text_transformer.fit_transform(train_df[\"clean_name\"])\nX_test = text_transformer.transform(test_df[\"clean_name\"])\n\nlogreg = LogisticRegression(C=0.1, max_iter=1000).fit(\n    X_train, train_df.label == \"person\"\n)\npreds = logreg.predict(X_test)\n\nbaseline_accuracy = accuracy_score(test_df.label == \"person\", preds)\nprint(f\"Baseline accuracy is {round(100*baseline_accuracy, 2)}%\")\n\nBaseline accuracy is 89.49%\n\n\n89.5% accuracy is not bad for a linear model! Remember, since the datasets are balanced, a baseline accuracy without any information would be 50%. Now, whether this is good or bad in an absolute sense, it depends on the actual application of the model. It also depends on the distribution of the words this model would actually see in production. The datasets I used are quite general, containing all kinds of people and company names. In a real application, the names could be more constrained (e.g. only coming from a particular country).\nNow, let’s see what mistakes the model makes (error analysis). It’s always interesting to look at examples where the model makes the worst mistakes. If we have a tabular dataset, it might be difficult to interpret what is going on, but, for perceptual data a human can understand (text, image, sound), this leads to invaluable insights into the model.\n\ntest_df[\"proba_person\"] = logreg.predict_proba(X_test)[:, 1]\ntest_df[\"abs_error\"] = np.where(\n    test_df.label == \"person\", 1 - test_df.proba_person, test_df.proba_person\n)\n\ntest_df.sort_values(\"abs_error\", ascending=False)[\n    [\"clean_name\", \"label\", \"proba_person\"]\n].head(10)\n\n\n\n\n\n  \n    \n      \n      clean_name\n      label\n      proba_person\n    \n  \n  \n    \n      60581\n      co co mangina\n      person\n      0.000206\n    \n    \n      49398\n      buster benton and the sons of blues\n      person\n      0.000984\n    \n    \n      6192\n      best horizon consulting\n      person\n      0.001613\n    \n    \n      83883\n      les enfants du centre de loisirs de chevreuse\n      person\n      0.002633\n    \n    \n      84646\n      manuel antonio nieto castro\n      company\n      0.997350\n    \n    \n      32669\n      chris joseph\n      company\n      0.996298\n    \n    \n      8545\n      hub kapp and the wheels\n      person\n      0.004568\n    \n    \n      77512\n      michael simon p.a.\n      company\n      0.994109\n    \n    \n      71392\n      dylan ryan teleservices\n      company\n      0.993017\n    \n    \n      64777\n      netherlands national field hockey team\n      person\n      0.007220\n    \n  \n\n\n\n\nWe can see that the mistakes are mostly understandable: There are many companies named just like people. How could the model know Chris Joseph is a company and not a person? The only way would be with information not available in the data I provided for its learning. We also see mislabelings in the people dataset: “netherlands national field hockey team” and “best horizon consulting” do not sound like people names!\nThis implies a high-leverage activity here would be cleaning the people dataset. If you want to make the data cleaning process sound sexier, just call it data-centric AI (just kidding: data-centric AI is actually a good framework to use for real-life machine learning applications where, in almost all cases, data trumps modelling)."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#fastai-lstm-fine-tuning",
    "href": "posts/name_classification/name_classification.html#fastai-lstm-fine-tuning",
    "title": "Name classification with ChatGPT",
    "section": "FastAI LSTM fine tuning",
    "text": "FastAI LSTM fine tuning\nFor the first more complex machine learning model, let’s start with FastAI due to is simple interface. Following the suggestion of this article, I used an AWD_LSTM model which was pre-trained as a language model that predicts the next word using Wikipedia as dataset. Then, I fine-tune the model with our classification problem. FastAI fine-tune works in the following way: in the first epoch, it only trains the head (the newly inserted neural network on top of the pre-trained language model), then, for all subsequent epochs, it trains the whole model together. FastAI uses many tricks to make the training more effective, which is all wrapped in a simple function call. While convenient, it makes understanding what is going on behind the scenes and any customization more difficult.\n\nfastai_df = pd.concat((train_df.assign(valid=False), test_df.assign(valid=True)))\ndls = TextDataLoaders.from_df(\n    fastai_df, text_col=\"clean_name\", label_col=\"label\", valid_col=\"valid\"\n)\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(5, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.513145\n      0.397019\n      0.802810\n      03:37\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.167889\n      0.137422\n      0.952030\n      07:53\n    \n    \n      1\n      0.157485\n      0.145000\n      0.956200\n      07:50\n    \n    \n      2\n      0.122625\n      0.139295\n      0.963160\n      07:53\n    \n    \n      3\n      0.112604\n      0.112886\n      0.968730\n      07:53\n    \n    \n      4\n      0.111916\n      0.111421\n      0.970460\n      07:53\n    \n  \n\n\n\nNow we ended with 97.1% accuracy, almost 8 percentage points higher than our baseline! Not bad for a few lines of code and one hour of GPU time. Can we do better? Let’s try using a 🤗 transformer."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#hugging-face-distilbert-classification-head-training",
    "href": "posts/name_classification/name_classification.html#hugging-face-distilbert-classification-head-training",
    "title": "Name classification with ChatGPT",
    "section": "Hugging Face DistilBERT classification head training",
    "text": "Hugging Face DistilBERT classification head training\nHugging Face offers hundreds of possible deep learning models for inference and fine-tuning. I chose DistilBERT due to time and GPU memory constraints. By default, Hugging Face trainer will fine-tune all the weights of the model, but now I just want to train the classification head, which is a two-layer fully-connected neural network (aka MLP). The reason is twofold: 1. We’re dealing with a simple problem and 2. I don’t want to leave the model training for too long to make reproducibility simpler and reduce GPU costs. I worked backwards from the previous results: Since FastAI took roughly one hour, I wanted to use the same GPU time budget here.\nTo only train the classification head, I had to use the PyTorch interface, which allows for more flexibility. First, I download DistilBERTs tokenizer, apply it to our dataset, then download the model itself, mark all layers as requiring no gradient (i.e. not trainable), and then train the classification head.\n\nbatch_size = 32\nnum_epochs = 3\nlearning_rate = 3e-5\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\nFirst, I apply the DistilBERT tokenizer to our datasets:\n\ntokenized_train_df = tokenizer(\n    text=train_df[\"clean_name\"].tolist(), padding=True, truncation=True\n)\ntokenized_test_df = tokenizer(\n    text=test_df[\"clean_name\"].tolist(), padding=True, truncation=True\n)\n\nNow, I create a PyTorch dataset that is the able to hand the input format given by the tokenizer (tokens + attention mask), alongside the labels:\n\ntrain_dataset = NamesDataset(\n    tokenized_train_df, (train_df.label == \"person\").astype(int)\n)\ntest_dataset = NamesDataset(tokenized_test_df, (test_df.label == \"person\").astype(int))\n\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2\n)\n\nI set all the base model parameters as non-trainable (so that only the classification head is trained):\n\nfor param in model.distilbert.parameters():\n    param.requires_grad = False\n\nFinally, I actually train the model (see full training and evaluation code in the notebook):\n\n\n\n\n\n\n\n\nepoch 0: test accuracy is 96.527%\n\n\n\n\n\nepoch 1: test accuracy is 96.776%\n\n\n\n\n\nepoch 2: test accuracy is 96.854%\n\n\nWe got 96.8% accuracy, essentially the same as FastAI LSTM model. This implies the extra complexity here was for nought. Of course, this problem is a simple one: If I had a more complex problem, I’m sure using a stronger pre-trained language model would give an edge relative to the simpler LSTM trained on Wikipedia. Also, by not fine-tuning the whole network, we miss out on the full power of the transformer. But this suggests that you shouldn’t write off FastAI without trying, which, as I show above, is quite simple.\nLet’s see which mistakes this model is making:\n\ntest_df[\"proba_person\"] = test_preds\ntest_df[\"abs_error\"] = np.where(\n    test_df.label == \"person\", 1 - test_df.proba_person, test_df.proba_person\n)\ntest_df.sort_values(\"abs_error\", ascending=False)[\n    [\"clean_name\", \"label\", \"proba_person\"]\n].head(10)\n\n\n\n\n\n  \n    \n      \n      clean_name\n      label\n      proba_person\n    \n  \n  \n    \n      6192\n      best horizon consulting\n      person\n      0.000008\n    \n    \n      47006\n      rolf schneebiegl & seine original schwarzwald-musi\n      person\n      0.000326\n    \n    \n      58512\n      development\n      person\n      0.000363\n    \n    \n      9404\n      xin yuan yao\n      company\n      0.999556\n    \n    \n      59585\n      cheng hsong\n      company\n      0.999490\n    \n    \n      46757\n      compagnie lyonnaise du cin ma\n      person\n      0.000550\n    \n    \n      38224\n      pawel siwczak\n      company\n      0.999389\n    \n    \n      25983\n      sarah hussain\n      company\n      0.999311\n    \n    \n      23870\n      manjeet singh\n      company\n      0.999295\n    \n    \n      73909\n      glassworks\n      person\n      0.000776\n    \n  \n\n\n\n\nAgain, we see cases of clear mislabeling in the case of person and some tough cases in the case of company. Given the accuracy and the worst mistakes, we may be at the limit of what can be done for this dataset without cleaning it. Now, the final question: Can I get the same level of accuracy without any supervised training at all?"
  },
  {
    "objectID": "posts/name_classification/name_classification.html#chatgpt-api-one-shot-learning",
    "href": "posts/name_classification/name_classification.html#chatgpt-api-one-shot-learning",
    "title": "Name classification with ChatGPT",
    "section": "ChatGPT API one-shot learning",
    "text": "ChatGPT API one-shot learning\nI will use OpenAI’s API to ask ChatGPT to do name classification for us. First, I need to define the prompt very carefully, what is now called prompt engineering. There are some rules of thumb for prompt engineering. For example, always give concrete examples before asking ChatGPT to generalize to new ones.\nThe ChatGPT API has three prompt types:\n\nSystem: Helps set the tone of the conversation and gives overall directions\nUser: Represents yourself, use it to state your task or need\nAssistant: Represents ChatGPT, use it to give examples of valid or reasonable responses\n\nYou can mix and match all prompt types, but I suggest starting with the system one, having at least one round of task-response examples, then restating the task that will actually be completed by ChatGPT.\nHere, I ask for ChatGPT to classify 10 names into person or company. If I ask for more, say 100 names, there is a higher chance of failure (e.g. it sees a weird string and complains there is nothing it can do regarding the whole batch). If there is still a failure, I do a backup query on each name individually. If ChatGPT fails to provide a clear answer on an individual name, I default to answering “company” since this class contains more problematic strings.\nFinally, how can I extract the labels from ChatGPT’s response? It might answer differently, for example, by fixing a misspelling or by using uppercase instead of lowercase (system prompt notwithstanding). In general, it answers in the same order, but can I rely on that completely for all 100k examples? To be safe, I do a simple string matching based on the Levenshtein distance to match the names I query with ChatGPT’s responses.\nTo reproduce the code below, you need to have an OpenAI account and OPENAI_API_KEY set in your environment\n\nsystem_prompt = \"\"\"\nYou are a named entity recognition expert.\nYou only answer in lowercase.\nYou only classify names as \"company\" or \"person\".\n\"\"\"\n\ntask_prompt = \"Classify the following names into company or person:\"\n\nexamples_prompt = \"\"\"google: company\njohn smith: person\nopenai: company\npedro tabacof: person\"\"\"\n\nbase_prompt = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": task_prompt},\n    {\"role\": \"assistant\", \"content\": examples_prompt},\n    {\"role\": \"user\", \"content\": task_prompt},\n]\n\nall_preds = []\n\n\ndef get_chatgpt_preds(batch_df):\n    \"\"\" Gets predictions for a whole batch of names using ChatGPT's API\"\"\"\n    prompt = base_prompt.copy()\n    prompt += [{\"role\": \"user\", \"content\": \"\\n\".join(batch_df.clean_name)}]\n\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    # Max tokens as 20000 is enough in practice for 10 names plus the prompt\n    # Temperature is set to 0 to reduce ChatGPT's \"creativity\"\n    # Model `gpt-3.5-turbo` is the latest ChatGPT model, which is 10x cheaper than GPT3\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\", messages=prompt, max_tokens=2000, temperature=0\n    )\n\n    # Since we gave examples as \"name: class\", ChatGPT almost always follows this pattern in its answers\n    text_result = response[\"choices\"][0][\"message\"][\"content\"]\n    clean_text = [\n        line.lower().split(\":\") for line in text_result.split(\"\\n\") if \":\" in line\n    ]\n\n    # Fallback query: if I cannot find enough names on the response, I ask for each name separately\n    # Without it, we'd have parsing failures once every 10 or 20 batches\n    if len(clean_text) < len(batch_df):\n        clean_text = []\n        for _, row in batch_df.iterrows():\n            prompt = base_prompt.copy()\n            prompt += [{\"role\": \"user\", \"content\": row.clean_name}]\n\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\", messages=prompt, max_tokens=2000, temperature=0\n            )\n\n            row_response = response[\"choices\"][0][\"message\"][\"content\"]\n            if \":\" in row_response:\n                clean_text.append(\n                    [row_response.split(\":\")[0], row_response.split(\":\")[-1]]\n                )\n            else:\n                clean_text.append([row.clean_name, \"company\"])  # defaults to company\n\n    # To ensure I'm matching the query and the corresponding answers correctly,\n    # I find the closest sentences in the Levenshtein distance sense\n    batch_df = batch_df.copy()\n    batch_df = batch_df.merge(pd.DataFrame({\"resp\": clean_text}), how=\"cross\")\n    batch_df[\"resp_name\"] = batch_df.resp.str[0].str.strip()\n    batch_df[\"resp_pred\"] = batch_df.resp.str[-1].str.strip()\n    batch_df[\"dist\"] = batch_df.apply(\n        lambda row: lev.distance(row.clean_name, row.resp_name), axis=1\n    )\n    batch_df[\"rank\"] = batch_df.groupby(\"clean_name\")[\"dist\"].rank(\n        method=\"first\", ascending=True\n    )\n    batch_df = batch_df.query(\"rank==1.0\")[[\"clean_name\", \"label\", \"resp_pred\"]]\n\n    return batch_df\n\n\nchatgpt_num_workers = 32\nchatgpt_batch_size = 10\nsplit_size = len(test_df) // chatgpt_batch_size\ntest_batches = np.array_split(test_df, split_size)\n\nchatgpt_preds = Parallel(n_jobs=chatgpt_num_workers, verbose=5)(\n    delayed(get_chatgpt_preds)(batch_df) for batch_df in test_batches\n)\n\n\nchatgpt_preds = pd.concat(chatgpt_preds)\nchatgpt_accuracy = (chatgpt_preds.resp_pred == chatgpt_preds.label).sum() / len(\n    chatgpt_preds\n)\n\nprint(f\"ChatGPT accuracy is {round(100*chatgpt_accuracy, 2)}%\")\n\nChatGPT accuracy is 97.52%\n\n\nIncredible! With 97.5% accuracy, ChatGPT managed to outperform complex neural networks trained for this specific task. One explanation is that it used its knowledge of the world to understand some corner cases that the models could not have possibly learned from the training set alone. In some sense, this would be a form of “leakage”: perhaps ChatGPT would be weaker classifying companies founded after its cutoff date (2021).\nChatGPT is also quite cheap to run: the cost to classify the 100k examples was just under $5. It took 18 min to score all the examples, which could probably be improved by better parallelism. If you don’t use any parallelism at all, it will be much slower.\nLet’s see the raw responses ChatGPT gives:\n\nchatgpt_preds.resp_pred.value_counts().head(20)\n\nperson                                                                                                                            50955\ncompany                                                                                                                           48913\ncompany or person (not enough context to determine)                                                                                  13\nit is not clear whether it is a company or a person.                                                                                 11\nneither (not a name)                                                                                                                  7\nnot a name                                                                                                                            6\nit is not clear whether this is a company or a person.                                                                                5\ncannot be classified as either company or person                                                                                      4\ncompany or person (not enough information to determine)                                                                               4\ni'm sorry, i cannot classify this name as it does not appear to be a valid name.                                                      3\nneither                                                                                                                               3\nit is not clear whether it is a person or a company.                                                                                  2\nn/a (not a name)                                                                                                                      2\ni am sorry, i cannot classify this name as it does not provide enough information to determine if it is a company or a person.        2\nthis is not a name.                                                                                                                   2\nthis is not a valid name.                                                                                                             2\nperson (assuming it's a misspelling of a person's name)                                                                               2\nneither person nor company (not a name)                                                                                               2\nperson or company (without more context it is difficult to determine)                                                                 2\nplace                                                                                                                                 2\nName: resp_pred, dtype: int64\n\n\nFor the vast majority of cases, ChatGPT answers as I request: person or company. In very rare cases, it states it doesn’t know, it’s not clear or could be both. What are such examples in practice?\n\nchatgpt_preds[~chatgpt_preds.resp_pred.isin([\"person\", \"company\"])].head(10)\n\n\n\n\n\n  \n    \n      \n      clean_name\n      label\n      resp_pred\n    \n  \n  \n    \n      55\n      alkj rskolen ringk bing\n      company\n      neither (not a valid name)\n    \n    \n      55\n      81355\n      person\n      cannot be classified without more context\n    \n    \n      22\n      telepathic teddy bear\n      person\n      neither\n    \n    \n      11\n      agebim\n      company\n      it is not clear whether it is a company or a person.\n    \n    \n      44\n      i quit smoking\n      company\n      neither company nor person\n    \n    \n      33\n      saint peters church\n      company\n      company (assuming it's a church organization)\n    \n    \n      88\n      holy trinity lutheran church akron oh\n      company\n      company (assuming it's a church organization)\n    \n    \n      55\n      displayname\n      company\n      company or person (not enough context to determine)\n    \n    \n      66\n      ken katzen fine art\n      company\n      company or person (not enough context to determine)\n    \n    \n      66\n      columbus high school\n      company\n      company (assuming it's a school)\n    \n  \n\n\n\n\nThe names ChatGPT cannot classify are definitely tricky, like “81355” or “telepathic teddy bear”. In some cases, like for “saint peters church”, it does get it right with some extra commentary in parenthesis. All in all, I’d say ChatGPT did an amazing job and failed in a very human way."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#conclusion",
    "href": "posts/name_classification/name_classification.html#conclusion",
    "title": "Name classification with ChatGPT",
    "section": "Conclusion",
    "text": "Conclusion\nI have explored 4 ways to classify names: from a simple logistic regression to a complex neural network transformer. In the end, a general API from ChatGPT outperformed them all without any proper supervised learning.\n\n\n\nMethod\nAccuracy (%)\n\n\n\n\nBaseline\n89.5\n\n\nBenchmark\n95\n\n\nFastAI\n97.1\n\n\nHugging Face\n96.9\n\n\nChatGPT\n97.5\n\n\n\nThere is a lot of hype around LLMs and ChatGPT, but I’d say it does deserve the attention it’s getting. Those models are transforming tasks that required deep machine learning knowledge into software + prompt engineering problems. As a data scientist, I’m not worried about those models taking over my job, as predictive modelling is only a small aspect of what a data scientist does. For more thoughts on this, check out The Hierarchy of Machine Learning Needs.\n\nAcknowledgements\nI’d like to thank Erich Alves for some of the ideas explored in this post. I also thank Erich and Raphael Tamaki for reviewing the post and giving feedback."
  },
  {
    "objectID": "posts/neurips_2018/index.html",
    "href": "posts/neurips_2018/index.html",
    "title": "NeurIPS 2018: A Data Scientist’s Perspective",
    "section": "",
    "text": "Two weeks ago in Montreal, NeurIPS (formerly known as NIPS) took place, the world’s largest conference on machine learning and artificial intelligence. Major advancements in the field were presented, covering Deep Learning, GANs, and Reinforcement Learning, including both theory and practice. Over eight thousand people attended, more than a thousand papers were accepted, and dozens of workshops were held. Additionally, nearly all major tech companies were present, primarily aiming to recruit scientists and researchers.\nI went to NeurIPS to present the work Adversarial Attacks on Variational Autoencoders at the LatinX in AI workshop, co-authored by George Gondim, myself and our professor Eduardo Valle. At NeurIPS 2017, Prof. Valle presented two works we wrote together: Adversarial Images for Variational Autoencoders at the Adversarial Training workshop and Known Unknowns: Uncertainty Quality in Bayesian Neural Networks at the Bayesian Deep Learning workshop. It’s a great feeling to finally be able to present the work myself!\nSpeaking of LatinX, this year diversity and inclusion were the hottest topics at the conference. We had three other workshops dedicated to this (Black in AI, Women in ML, Queer in AI), as well as smaller talks and gatherings on the subject. Despite this, more than half of the Black in AI participants couldn’t obtain visas in time or were denied, which had some repercussions and may influence future conference locations. At LatinX, the Latin American artificial intelligence meeting Khipu was announced, scheduled to take place in Montevideo in November 2019. We Brazilians cannot miss this opportunity!\nThe conference had its issues: tickets sold out in just 11 minutes; last-minute name change (and we were left without mugs because of it); overcrowded sessions, with people being kicked out due to “fire hazards”; numerous audiovisual problems (it would have been better to watch many talks from home). Despite all of this, the experience is certainly unique: there’s no other place in the world where you might bump into Geoff Hinton, Yoshua Bengio, and Yann LeCun in the halls. Of course, they’ll always be surrounded by fans and people wanting to take photos!\n\n\n\nImagine you’re explaining your poster and Geoff Hinton appears behind you?\n\n\nAlthough these “celebrities” were present, their students were the ones presenting the academic work. With over a thousand papers at the main conference and hundreds at the workshops, it’s hard to know be aware of everything that happened there. I noted down more than 30 articles to review more carefully later. Although this is a large amount to read, it’s only a fraction of the total. For those who want a taste of what’s published there, here are some examples:\nBest papers (according to reviewers and program committee):\n\nNeural Ordinary Differential Equations\nNon-delusional Q-learning and Value-iteration\nOptimal Algorithms for Non-Smooth Distributed Optimization in Networks\nNearly Tight Sample Complexity Bounds for Learning Mixtures of Gaussians via Sample Compression Schemes\n\nBest papers (according to me):\n\nRegularization Learning Networks: Deep Learning for Tabular Datasets\nBayesian Neural Network Ensembles\nThe Everlasting Database: Statistical Validity at a Fair Price\nHow to Start Training: The Effect of Initialization and Architecture\n\nBesides the academic side, NeurIPS has a strong social aspect: parties and gatherings. Excluding the official closing, the parties were all sponsored by companies. I attended two (Nvidia and Element AI), with excellent food and drinks. There’s a bit of elitism: I only managed to attend these two parties thanks to a friend who works with big names in a Montreal lab. However, every day there were several options, with varying degrees of difficulty to get a ticket.\nIn my opinion, the gatherings organized by participants through the conference app were the best part of the week, as they brought together people with similar interests and objectives. I attended AI for Business and AI in Production, both highly relevant to my work as a data scientist at Nubank. At those events, it’s possible to meet people from various backgrounds and profiles who share something with you, without the elitism of those exclusive parties.\n\n\n\nAI for Business lunch (I’m wearing purple in the top right photo)\n\n\nFinally, the last two days were dedicated to workshops. In them, you could delve deep into a specific topic and even learn about things not officially published anywhere yet. The workshop that left the greatest impression on me was AI in Financial Services. However, it had a strong bias toward North American and European realities, mostly discussing regulations and how machine learning could be done within these constraints. For example, in those countries, you need to explain the reasons for denying a loan, but how do you do that if the loan risk is calculated by a deep neural network? However, many countries like China, India, and Brazil don’t have such restrictions, so our challenges are different and were barely explored there. I can say that Nubank is at the forefront of applying machine learning to financial products globally.\nNeurIPS is an academic conference, not the best place to meet other data scientists, but rather the best place to find the top machine learning and AI researchers. I recommend the experience for those with an academic inclination, who have the habit of reading papers and plan to or have already published work in the field. If you have more practical interests, there are conferences and meetings that may be more useful professionally and more accessible, such as Strata, PAPIs.io or even KDD. The most important thing is to get out there and meet new people, but don’t forget to take some time to explore and enjoy the trip!"
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html",
    "href": "posts/hierarchy_needs_ml/index.html",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "",
    "text": "In 1943, Abraham Maslow created the hierarchy of human needs, ranging from basic physiological needs to abstract concepts like self-actualization. In this article, I propose a hierarchy of machine learning needs:\nA framework like this can be useful for answering questions like:\nI try to answer these questions and more at the end of this article, but first, it’s necessary to define and better understand each need in the hierarchy."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#the-hierarchy-of-needs",
    "href": "posts/hierarchy_needs_ml/index.html#the-hierarchy-of-needs",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "The hierarchy of needs",
    "text": "The hierarchy of needs\n\nBusiness\nBusiness sits at the base of the pyramid, as it’s the foundation everything is built upon. Directly or indirectly, data scientists must always strive to deliver value to the business. This way, we can impact customers positively, secure resources (whether human or computational), and advance in our careers. This pragmatic view may be discouraging for those seeking technical challenges only, but I propose that business challenges are more difficult and unique than those found in machine learning competitions.\nNo e-commerce company aims solely to predict customer churn; the real goal is to take actions that reduce churn among the most valuable customers. Credit risk assessment alone isn’t very useful; deciding who will receive credit and for how much is the core of many financial institutions. The probability of lead conversion is just the first step in prioritizing and allocating sales resources in a B2B company. In all three examples, the focus is on intervention (causality) rather than just prediction (machine learning).\nI worked on a document classification project where there initially seemed to be no complicated business issues involved; achieving sufficient accuracy would ensure the project’s success. After talking to the actual users, I realized that the classification rationale was more important than the decision itself. In the same place, a project had previously failed for considering only the model’s AUC without any concern for its practical use. We shifted our focus and delivered a system that was genuinely useful for the users and the company by investing more in the UI and less in the modelling.\n\n\nTarget\nThe target is what the model will try to predict, which may not be obvious at first:\n\nChurn: What should be the time window to define churn? What if the user becomes active again?\nCredit: How long must one go without paying to be a default? What if the collections team gets all the money back?\nSales lead: At what point in the sales funnel do we define conversion? What if there is a refund?\nDocument classification: What should be done with sub-categories? Can we group smaller categories into “others”?\n\nChoosing the target is the most critical step in modelling. No features or models can save an inappropriate target. On the other hand, having an appropriate target allows even very simple models (such as linear regression) with basic features to have some impact and already be deployed to production.\n\n\nEvaluation/Metrics\nEvaluation is the framework that objectively assesses how well a model will perform when deployed. The evaluation process should closely resemble what happens after the model’s deployment. While the standard holdout or cross-validation splits provide a starting point, they are generally insufficient. If the problem evolves over time (as is the case for nearly all business problems), a time-series split should be employed. If the target is censored for six months, there should be a six-month gap between training and testing. If the data contains groups and the model will make predictions for new groups in the future, you should use a group split.\nMetrics judge how well a model is predicting its target. It’s not uncommon to use more than one evaluation metric for the same problem. For example, in a binary classification problem, one might use AUC to assess how well the model ranks examples and log loss to evaluate whether the probabilities are well calibrated. It’s also common to consider business metrics, such as expected conversion rate or the number of credit approvals. These metrics are harder to estimate offline and generally require some assumptions or experiments. Although handling these metrics may be more difficult, they serve as a more powerful guide than traditional machine learning metrics. Plus, communicating results with people from other areas becomes much easier!\n\n\nFeatures\nFeatures are the inputs of the model. They need to be predictive of the target. It’s crucial to avoid leakage, meaning that when deploying the model in production, features must appear in the same way they did during training. I’ve encountered leakage in a conversion prediction project where I used features that only appeared when the user converted. In other cases, the values were missing. The model learned that missing values were never associated with conversions and achieved a 100% AUC. However, this model had no real value for the business!\nFeature engineering can be partially automated with tools like Featuretools. However, most of the work in creating new features depends on understanding the problem and the available data (including what can be crawled or purchased externally).\nAre more features always better? Not necessarily. More features may require more monitoring and engineering, which may not be a good trade-off in certain cases. It’s essential to balance the value of features (possibly with a business metric) with their operational and maintenance costs.\n\n\nModels\nIn the end, given the constraints of business, target, evaluation/metrics, and features, the choice of models narrows considerably. If the business requires interpretability of predictions, you shouldn’t use a neural network. If the target is continuous, you want a regression model, not a classification model. If the metric evaluates the calibration of probabilities, you want a model that can learn a proper scoring rule. If you have more features than examples, you want a model that can ignore most features, like Lasso regression.\nThe modelling process can be automated with tools like auto-sklearn or PyCaret, but only if it makes sense for the business. In some cases, gaining an additional percentage point in accuracy is less useful than having interpretable decisions, communicating with other areas about how the model works (including external regulators), training speed for “big data” cases, and prediction latency for real-time systems."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#applications",
    "href": "posts/hierarchy_needs_ml/index.html#applications",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "Applications",
    "text": "Applications\nWe can apply the hierarchy of needs to better understand the reality of machine learning, which is difficult to learn solely from MOOCs or competitions:\n\nThe Spiral of Applied Machine Learning\nIn practice, machine learning is not a linear process, starting with a business problem and ending with a model. The process is repeated several times, and each iteration feeds the next:\n\nThis suggests starting simple on the modelling side. Always define a baseline first, which can be a business rule of thumb or a simple model (e.g. a linear regression or a decision tree classifier). The first proper model you build to compare against the baseline should be pragmatic, which depends on your domain:\n\nTabular data: Use LightGBM (watch my PyData London presentation here) or XGBoost\nTime series forecasting: Use Auto-ARIMA or Prophet\nText classification: Use word counts and Naive Bayes, TF-IDF and logistic regression, or ChatGPT API\nImage recognition: Use ResNet with transfer learning or ChatGPT4 API\n\nThis will handle the “model” part of the spiral and probably capture most of the gains. Use your time to focus on the other components before coming back to the model: deploy it as soon as possible, get feedback from the stakeholders, explore the failure modes (error analysis), and then go over the target-evaluation-metrics-features-model process again.\n\n\nAuto ML\nMachine learning automation operates only at the highest levels of the pyramid: primarily in modelling, followed by feature engineering and selection. The other needs are much harder to automate and are the biggest differentiators among data scientists since they depend on domain knowledge. Even in the modelling part, there are cases where maximizing a single metric does not capture the whole story:\nOnce, while I was in the process of updating models, an analyst noticed that there was a small sub-population where the new model provided predictions that made no sense. In terms of metrics, there was no doubt that the new model was better, but for the business, it was not good to make mistakes in this sub-population. In this case, since I was using a relatively interpretable model, I managed to discover the reason for the incorrect predictions (related to how the missing values were being handled) and could deploy an appropriate solution.\n\n\nMonitoring\nMonitoring a model in production should be done at all levels of the hierarchy. Monitoring is generally associated with evaluation metrics, but other needs should not be ignored. For example, in a credit card fraud problem, the target is generally censored for several months (since it takes time to determine whether fraud has occurred or not, as it is a manual process), meaning that metrics can only be calculated months after each model decision. In this case, it is important to evaluate how the target is changing over time (using proxies with shorter censoring), monitor whether the model’s output distribution remains stable over time, and whether the distribution of features remains the same, which is a significant challenge in itself. For more information on model monitoring, I highly recommend watching Lina Weichbrodt’s PyData Berlin presentation.\n\n\nKaggle\nIn the case of Kaggle, the entire challenge lies in modelling, feature engineering and evaluation. The metric and target are already given. The business aspect is not explicitly present. With this, we can see the limitations of Kaggle as training and evaluation for a data scientist who will work on real-world problems.\nKaggle is a great tool for its purpose and is highly valued in the selection process for some positions. However, a data scientist needs to go beyond this and try to tackle other types of problems, those that are not well-formulated and therefore are fertile ground for exploring targets, metrics, and the use of predictive models in decision-making."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#conclusion",
    "href": "posts/hierarchy_needs_ml/index.html#conclusion",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, I tried to convey the perspective of a machine learning practitioner, undoubtedly biased by my own particular experience. However, I believe the points raised are relevant to many other data scientists, especially those coming from an academic background. I hope you can use the hierarchy of needs to better guide decisions made as practitioners or students in the field.\nThis blog post originally appeared in the Data Hackers Medium in 2019. Full disclosure: the translation was done with the help of GPT4. I reviewed the final text and updated some sections, since I’ve personally learned and grown a lot since 2019. I’m open to feedback or suggestions for more applications of the hierarchy of needs framework."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Name classification with ChatGPT\n\n\nHow does it compare to machine learning language models?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nThe Hierarchy of Machine Learning Needs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nNeurIPS 2018: A Data Scientist’s Perspective\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2018\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nHow (not) to forecast an election\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2016\n\n\nPedro Tabacof\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Personal blog of Pedro Tabacof.\nReach out to me via Linkedin, Twitter, or last name at gmail dot com."
  }
]