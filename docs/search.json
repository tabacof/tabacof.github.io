[
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "",
    "text": "In this post, I will explore how to deploy a real-time machine learning model using AWS Lambda and API Gateway. I will go over the following points:\nIf you just want the reproducible code, check out the raw notebook here.\nBefore we get to the technical details, why did I decide to write a post and make a PyData presentation about serverless deployment?"
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#motivation",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#motivation",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "Motivation",
    "text": "Motivation\n\nFirst data scientist role\nIn my first data scientist role, I was tasked with shipping a credit card fraud model. Long story short, I built a good enough model using PySpark that would have significant impact on the bottom line ($). To deploy it, a machine learning engineer (MLE) had to work full-time for months to build a model serving framework using Scala. To bridge the gap between Python and Scala, the models had to be serialized using MLeap. Why not use a Python server, something I had experience with? The company simply wouldn’t allow non-JVM servers in prod! There had to be a better way.\n\n\nFailed side hustle\nLater, I learned about AWS Lambda and realized I could have deployed the fraud model using it. It would run Python code, but not need a Python server or any associated infrastructure. This led me to the idea of creating a side hustle called Deploir:\n\n\n\nLanding page of Deploir\n\n\nThe idea was simple: a data scientist trains a model, generates a pickle, uploads the pickle to the website using a GUI, and 1 minute later they have an endpoint ready to be used at scale. Technically, the idea was sound and the prototype worked. Commercially, I couldn’t get one single conversion. Deploir taught me many valuable lessons about overbuilding (see this tweet for a hilarious and maximalist view). But it also taught me how to deploy real-time models using serverless tech, which I intend to share now."
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#summary",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#summary",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "Summary",
    "text": "Summary\nHere is a summary of what we’re about to explore in this post:\n\n\n\nFrom model to endpoint in 3 simple steps\n\n\nI will show how to go from a Scikit-learn model (but could be XGBoost, LightGBM, Tensorflow, etc) to a REST endpoint in three steps: Pickle the model, create a Lambda function to serve that model pickle, then wrap the function around a POST endpoint using API gateway.\nBefore we get to the engineering, first I’d like to discuss different model deployments modes (real-time vs batch) to make it clear what I mean by “real-time”, how Lambda and API gateway work and why serverless might be a good idea for some use cases and bad for others."
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#real-time-vs-batch-models",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#real-time-vs-batch-models",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "Real-time vs batch models",
    "text": "Real-time vs batch models\n\nReal-time models\nReal-time models serve predictions (almost) instantly. Usually, they serve one prediction at a time as they come. The prototypical real-time ML service in Python is a Flask server running a /predict endpoint that handles POST requests. After receiving a request with features in the payload, the server will process the features (if needed), call the model prediction function (e.g. model.predict(X)) and return the prediction as a JSON HTTP response. Then, the main application will do something with it, like approve or deny a loan. If something fails, the application returns a HTTP error. Check out a minimal example.\n\n\n\nTypical real-time ML application using Flask\n\n\nThere are many other possibilities to serve real-time predictions. You can use a different programming language other than Python by serializing the model in a language-agnostic framework like ONNX. You can use FastAPI instead of Flask, which is becoming more common over time as FastAPI is blazing fast with great Pydantic integration. Or you might use a queue (e.g. Kafka) and handle requisitions asynchronously. The details change, but the principles are similar.\nReal-time models have a couple of strong downsides:\n\nNeed to build, manage and maintain an endpoint or web server\nNeed to guarantee the feature engineering pipelines are the same for training and serving (which is more difficult than it seems!)\n\nBut it’s inevitable for some applications where the most informative features are only received in real-time:\n\nCredit card transaction fraud\nReal-time bidding for ad auctions\nChatbots\n\nBe sure to test your need for a real-time model empirically: train a model with just day old features and see how its performance compares to a model with real-time features. You might be surprised by how far “stale” features go.\n\n\nBatch models\nBatch models serve predictions in batch (duh) and generally at a regular frequency, typically every 24h. Those models are usually part of the data pipeline or ETL process. The prototypical batch ML architecture in Python is a Spark pipeline controlled by Airflow (or Snowflake controlled by DBT) that reads from production databases, makes all kind of data transformations, and somewhere along the way calls the model to make predictions, which are then pushed back to a production database to be used by the main application:\n\n\n\nTypical batch ML application using Airflow/Spark\n\n\nUse batch models if you can: you train models in batch after all! With batch models, it’s much easier to ensure the feature pipelines for training and serving are the same. You don’t have to maintain a web server or endpoint, only one or a few steps in the data pipeline. You can leverage everything the data engineers build in your favour.\nWhat are some typical batch model applications?\n\nPredicting churn in a B2B SaaS company\nCalculating credit scores\nEstimating Lifetime Value (LTV) of users\n\n\n\nHybrid models\nSince real-time models are sometimes inevitable (but be sure to test this assumption!) and batch models are easier to maintain, you can get the best of both worlds by chaining them together. For example, for a loan application, you can have a credit risk model that runs in batch based on past user behavior and external information like credit scores and another model running in real-time that uses the batch model as a feature plus other variables which are only available online (such as the loan amount). This offloads most of the complexity to the batch model, with the downside of having two models instead of one and having one depend on the other (so watch out for overfitting and leakage if the models re-use the same training examples)."
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#aws-lambda",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#aws-lambda",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "AWS Lambda",
    "text": "AWS Lambda\nAWS Lambda allows you to run functions on the cloud without having to manage webservers. You send a Python function to AWS and, one minute later, can invoke it from anywhere. Lambda has strong but not unlimited scalability, so you generally don’t have to worry about up or downscaling servers to handle variable load.\nMost importantly, you only pay for what you use. The cost is proportional to the time spent in each function call. The price varies by other things such as memory used and network traffic, but you can use the following rule-of-thumb as a ballpark estimate for a typical workload: ~1M requests → $1.\nYou can create Lambda functions manually in the web interface or programatically using boto3. I strongly recommend the latter: you get reproducibility and more explicit control. There are two ways to create a Lambda function:\n\nZipping your script and artefacts (including library dependencies) into a file\nCreating a Docker image with your script and artefacts\n\n\n\n\nLambda deployment choices: go with Docker\n\n\nI highly recommend going with option (2): Python libraries for ML are quite large and will create an unreasonably large zip file, which may exceed the standard Lambda limits. With a Docker image, you can install libraries with a requirements.txt file and the limits are much looser (the image can be up to 10g). Also, a Docker image is reproducible with a Dockerfile and allows for local testing with the exact same behaviour as production.\nWhen I first built Deploir, only zipping was available. One of my technical breakthroughs was compressing the numerical and ML libs to make them fit the file size and memory constraints. I hope nobody else has to experience the same pain again!"
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#api-gateway",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#api-gateway",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "API Gateway",
    "text": "API Gateway\nAPI Gateway allows you to wrap an endpoint around a Lambda function. There is no code involved, only configuration. You just specify you want to create a REST API with a POST endpoint using a certain Lambda function and the integration is done by AWS. You get one million requests for free every month and for subsequent use you pay $3.5 per million requests.\nUse API Gateway to distribute and control usage of Lambdas inside and outside your org. You can define API keys to use the endpoint with associated usage plans where you can set rate limits, throttling and quotas. The integration between API gateway and Lambda is seamless: it will pass along the request payload to the Lambda function event argument and it will return the request response from the Lambda return dictionary:\n\n\n\nAPI gateway: the lifetime of a request\n\n\nNote that there are two situations where you wouldn’t need to use API gateway:\n\nIf you only call the Lambda inside your org, so you can invoke it directly with boto3 or another AWS library\nIf you use Lambda function URLs to distribute your Lambda, which I wouldn’t recommend since API Gateway is already simple enough to use, quite cheap, and offers you more control"
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#trade-offs",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#trade-offs",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "Trade-offs",
    "text": "Trade-offs\nGoing serverless has many trade-offs. Check out When (and why) not to go serverless for a more general take. For ML deployment, we can be more precise about the trade-offs we face:\n\nWhy serverless model deployment?\nYou should go serverless if you cannot bear the costs of maintaining a Python server, whether they are the server cloud costs if you’re a poor entrepreneur, the human-engineering costs if you need to build a fancy Scala server (when Python is not allowed), or the time costs of managing a server yourself. Here are some examples that all have applied to me personally to go with Lambda in the past:\n\nYou are a data scientist (DS) and need to deploy a model but Python servers are not allowed\nYou are an indie hacker and want to sell something with high scalability but no baseline costs\nYou are a MLE and want to create a system where DSs send a model and get an endpoint back in seconds\n\n\n\nWhy not?\nYou should choose boring technology by default. If you can use Flask for your real-time model, do it! There will be a lot more resources, examples, and people with tacit knowledge and experience out there.\nIf your application has tight latency requirements, say you work with real-time ad auctions, the latency spikes from Lambda cold start might cause issues. You can prevent this by keeping the function warm manually, but this is hackish without any guarantees from AWS. You can provision concurrency, but you will pay for it, so the solution becomes less cost-effective.\nIf you operate at absurd scales like Prime Video, Lambda might not be cost-effective anymore since there are slight inefficiencies in the serverless model that are only apparent at scale. However, if the load varies a lot and unpredictably, Lambda can still outperform traditional web servers.\nIf you are at the limits of technology, say running LLMs inference on GPUs, it’s not even possible to start using Lambda in the first place."
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#scikit-learn-text-classifier",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#scikit-learn-text-classifier",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "Scikit-learn text classifier",
    "text": "Scikit-learn text classifier\nLet’s build a text classifier using Scikit-learn. If you just want to see the AWS part, skip to the next section. First, I load the 20 Newsgroups dataset:\n\ntrain_dataset = fetch_20newsgroups(subset='train')\ntest_dataset = fetch_20newsgroups(subset='test')\n\n\nprint(train_dataset.DESCR[:1086])\n\n.. _20newsgroups_dataset:\n\nThe 20 newsgroups text dataset\n------------------------------\n\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\n20 topics split in two subsets: one for training (or development)\nand the other one for testing (or for performance evaluation). The split\nbetween the train and test set is based upon a messages posted before\nand after a specific date.\n\nThis module contains two loaders. The first one,\n:func:`sklearn.datasets.fetch_20newsgroups`,\nreturns a list of the raw texts that can be fed to text feature\nextractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\nwith custom parameters so as to extract feature vectors.\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\nreturns ready-to-use features, i.e., it is not necessary to use a feature\nextractor.\n\n**Data Set Characteristics:**\n\n    =================   ==========\n    Classes                     20\n    Samples total            18846\n    Dimensionality               1\n    Features                  text\n    =================   ==========\n\n\nTo make the model outputs more interpretable and illustrative, I will only work with the classes as strings (e.g. \"rec.sport.hockey\") instead of numerical labels (e.g. 3).\n\nX_train = train_dataset.data\ny_train = np.array([train_dataset.target_names[i] for i in train_dataset.target])\n\n\nX_test = test_dataset.data\ny_test = np.array([test_dataset.target_names[i] for i in test_dataset.target])\n\n\nprint(f\"Train size: {len(X_train)}, test size: {len(X_test)}\")\n\nTrain size: 11314, test size: 7532\n\n\nNow, let’s build our model. Modelling is not so important for the purpose of this post, but TF-IDF + Logistic Regression is always a good idea for text classification. This is a bag-of-words approach: we will treat the words independently and the model will have to makes its predictions based on the presence or not of certain keywords. Logistic regression works well for sparse features, which is the outcome of a bag-of-words featurization. Alternatively, another strong baseline would be Naive Bayes.\n\ntfidf = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english', use_idf=True)\nlr = LogisticRegression(C=1e2, solver='lbfgs', multi_class='multinomial', max_iter=1000, random_state=42, n_jobs=16)\nle = preprocessing.LabelEncoder()\n\n\npipe = Pipeline([('tfidf', tfidf), ('lr', lr)])\n\n\npipe.fit(X_train, le.fit_transform(y_train))\n\nPipeline(steps=[('tfidf',\n                 TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')),\n                ('lr',\n                 LogisticRegression(C=100.0, max_iter=1000,\n                                    multi_class='multinomial', n_jobs=16,\n                                    random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('tfidf',\n                 TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')),\n                ('lr',\n                 LogisticRegression(C=100.0, max_iter=1000,\n                                    multi_class='multinomial', n_jobs=16,\n                                    random_state=42))])TfidfVectorizerTfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')LogisticRegressionLogisticRegression(C=100.0, max_iter=1000, multi_class='multinomial', n_jobs=16,\n                   random_state=42)\n\n\n\npreds = pipe.predict(X_test)\nground_truth = le.fit_transform(y_test)\nprint(f\"Accuracy is {100.0*accuracy_score(preds, ground_truth)}%\")\n\nAccuracy is 84.59904407859798%\n\n\nWe have 84.6% accuracy, but is that good? Accuracy is highly sensitive to class balance, so let’s take a look at the class proportions of the test set:\n\npd.Series(y_test).value_counts(normalize=True)\n\nrec.sport.hockey            0.052974\nsoc.religion.christian      0.052841\nrec.motorcycles             0.052841\nrec.sport.baseball          0.052708\nrec.autos                   0.052576\nsci.crypt                   0.052576\nsci.med                     0.052576\ncomp.windows.x              0.052443\nsci.space                   0.052310\ncomp.os.ms-windows.misc     0.052310\nsci.electronics             0.052177\ncomp.sys.ibm.pc.hardware    0.052045\nmisc.forsale                0.051779\ncomp.graphics               0.051646\ncomp.sys.mac.hardware       0.051115\ntalk.politics.mideast       0.049920\ntalk.politics.guns          0.048327\nalt.atheism                 0.042353\ntalk.politics.misc          0.041158\ntalk.religion.misc          0.033324\nName: proportion, dtype: float64\n\n\nGreat, this means choosing the most common class would only yield 5% accuracy. We can also look at the literature and see that a much more sophisticated approach using Deep Learning achieves 89%, not that far from our simple bag-of-words + linear classifier. Now let’s take a look at some predictions:\n\npipe.predict(X_test)\n\narray([ 7,  1,  0, ...,  9, 12, 15])\n\n\nOh no, we’re getting numbers instead of actual classes. For deployment, we don’t want the classifier to spit a number between 0 and 19. If it’s a customer facing endpoint, we’d rather make an actual prediction a human can read. To do so, we will need to augment our pipeline with an inverse label encoder which maps numeric labels into topics strings.\nCreating new Scikit-learn transformers / estimators to do so is quite simple:\n\nclass InverseLabelEncoder(TransformerMixin, BaseEstimator):\n    def __init__(self, le):\n        super().__init__()\n        self.le = le\n                 \n    def fit(self, _, y):\n        self.le.fit(y)\n        return self\n                 \n    def predict(self, y):\n        return self.le.inverse_transform(y)\n\nSince the Logistic Regression will not be the last step of the pipeline anymore, we need to create an “Internal Logistic Regressor”, which behaves as an intermediary pipeline step i.e. it uses the transform method instead of the predict one.\n\nclass InternalLR(TransformerMixin, BaseEstimator):\n    def __init__(self, lr):\n        super().__init__()\n        self.lr = lr\n                 \n    def fit(self, X, y):\n        self.lr.fit(X, y)\n        return self\n\n    def transform(self, X):\n        return self.lr.predict(X)\n\nNow, all we need to do is create a new pipeline that is able to spit out a topic instead of a number:\n\ninv_le = InverseLabelEncoder(le)\nilr = InternalLR(lr)\npredict_pipe = Pipeline([('tfidf', tfidf), ('ilr', ilr), ('inv_le', inv_le)])\npredict_pipe\n\nPipeline(steps=[('tfidf',\n                 TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')),\n                ('ilr',\n                 InternalLR(lr=LogisticRegression(C=100.0, max_iter=1000,\n                                                  multi_class='multinomial',\n                                                  n_jobs=16,\n                                                  random_state=42))),\n                ('inv_le', InverseLabelEncoder(le=LabelEncoder()))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('tfidf',\n                 TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')),\n                ('ilr',\n                 InternalLR(lr=LogisticRegression(C=100.0, max_iter=1000,\n                                                  multi_class='multinomial',\n                                                  n_jobs=16,\n                                                  random_state=42))),\n                ('inv_le', InverseLabelEncoder(le=LabelEncoder()))])TfidfVectorizerTfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')ilr: InternalLRInternalLR(lr=LogisticRegression(C=100.0, max_iter=1000,\n                                 multi_class='multinomial', n_jobs=16,\n                                 random_state=42))lr: LogisticRegressionLogisticRegression(C=100.0, max_iter=1000, multi_class='multinomial', n_jobs=16,\n                   random_state=42)LogisticRegressionLogisticRegression(C=100.0, max_iter=1000, multi_class='multinomial', n_jobs=16,\n                   random_state=42)inv_le: InverseLabelEncoderInverseLabelEncoder(le=LabelEncoder())le: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()\n\n\n\npredict_pipe.predict(X_test)\n\narray(['rec.autos', 'comp.graphics', 'alt.atheism', ...,\n       'rec.sport.baseball', 'sci.electronics', 'soc.religion.christian'],\n      dtype='&lt;U24')\n\n\nSuccess! Since this post-processing is part of the Scikit-learn pipeline, this means we can bundle all of it together in the deployment process. But first, let’s check we still have the same performance:\n\nprint(f\"Accuracy is {100.0*accuracy_score(predict_pipe.predict(X_test), y_test)}%\")\n\nAccuracy is 84.59904407859798%\n\n\n\nInterpretability with LIME\nBefore we get to the model deployment, let’s take a look at what the model is doing using LIME. It’s always important to develop an intuition for what the model is doing before using it.\n\nexplainer = LimeTextExplainer(class_names=test_dataset.target_names)\n\n\nlime_idx = 2\n\n\nexp = explainer.explain_instance(X_test[lime_idx], pipe.predict_proba, num_features=6, top_labels=2)\n\n\nexp.show_in_notebook(text=X_test[lime_idx])\n\n\n\n        \n        \n        \n        \n        \n        \n\n\n\nprint(f\"Actual class: {y_test[lime_idx]}\")\n\nActual class: alt.atheism\n\n\nWe can see that presence of keywords such as Atheism, God and Mathew (sic) make the model predict it’s an Atheism mailing list, which unsurprisingly is correct. The fact that we are using an email alias as a predictor shows the limitations of this model and dataset.\n\n\nPickle\nFinally, let’s save this model for deployment using Cloudpickle. Note that you cannot use the standard library Pickle in this case, since we created a custom class! Cloudpickle works the same way as the regular pickle, but it saves custom classes as well in the pickle. Note that pickles are in general an unsafe form of model sharing since they allow for execution of arbitrary code. Only open pickles of good provenance.\nIf pickling is infeasible or too unsafe for your use case, you can use a model serialization framework like the aforementioned MLeap and ONNX.\n\nfile_name = 'model.pickle'\n\n\nwith open(file_name, 'wb') as handle:\n    cloudpickle.dump(predict_pipe, handle)"
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#iam-role",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#iam-role",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "IAM role",
    "text": "IAM role\nIn a real application, you probably want to use Terraform or AWS CloudFormation to manage your infrastructure and cloud permissions. For our standalone example, we can create manually an IAM role that will allow the creation and invocation of Lambdas:\n\niam_client = boto3.client('iam', region_name=region)\n\n\n# Specify the role name and trust policy for the Lambda service\nrole_name = 'lambda-exec-role'\n\ntrust_policy = {\n    'Version': '2012-10-17',\n    'Statement': [\n        {\n            'Effect': 'Allow',\n            'Principal': {'Service': 'lambda.amazonaws.com'},\n            'Action': 'sts:AssumeRole'\n        }\n    ]\n}\n\n\n# Just need to run it once, otherwise retrieve already existing role\ntry:\n    response = iam_client.get_role(\n        RoleName=role_name\n    )\nexcept iam_client.exceptions.NoSuchEntityException:\n    response = iam_client.create_role(\n        RoleName=role_name,\n        AssumeRolePolicyDocument=json.dumps(trust_policy),\n        Description='Execution role for Lambda function',\n    )\n\n\n# Get the role ARN\nrole_arn = response['Role']['Arn']\n\n# Attach the AWSLambdaBasicExecutionRole policy to the role\npolicy_arn = 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\nresponse = iam_client.attach_role_policy(\n    RoleName=role_name,\n    PolicyArn=policy_arn\n)"
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#lambda",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#lambda",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "Lambda",
    "text": "Lambda\nAs explained before, there are two ways to create a Lambda function: just zipping all the relevant files together or via a Docker image. The easiest way in general is using a zip file, but the standard Python image won’t contain the necessary libraries for ML such as Numpy or Scikit-learn. While possible to add the libraries to the zip, it’s easier to use a Dockerfile.\nWe just need to define a Dockerfile, a requirements.txt and the predict.py script that will actually be called by the Lambda:\n\nDockerfile: the Docker image definition\nrequirements.txt: the libraries used to train your model\npredict_sklearn.py: the script used in the Lambda function\n\nThe most interesting part is the predict_sklearn.py function:\nimport json\nimport cloudpickle\n\n# By loading the pickle outside `predict`,\n# we re-use it across different Lambda calls for the same execution instance\nwith open('model.pickle', 'rb') as f:\n    model = cloudpickle.load(f)\n\ndef api_return(body, status):\n    return {\n        'isBase64Encoded': False,\n        'statusCode': status,\n        'headers': {'Content-Type': 'application/json'},\n        'body': json.dumps(body, default=str)\n    }\n\ndef predict(event, context):\n    if isinstance(event['body'], str):\n        try:\n            payload = json.loads(event['body'])\n        except json.JSONDecodeError:\n            return api_return({'error': 'JSON decode error when decoding payload'}, 400)\n    elif isinstance(event['body'], list):\n        payload = event['body']\n    else:\n        return api_return({'error': 'Unknown input format'}, 400)\n\n    # Scikit-learn needs a list or array as input\n    if not isinstance(payload, list):\n        payload = [payload]\n\n    try:\n        output = model.predict(payload).tolist()\n    except Exception as e:\n        return api_return({'error': str(e)}, 500)\n\n    return api_return(output, 200)\nMost of the function above is error handling. Essentially, we are loading a model pickle outside the function (so that it’s cached across multiple calls) and calling model.predict inside it.\n\nDocker image\nWe need to login to ECR (Elastic Container Registry), build the image, tag and then and push it to the registry. Then, this image will be accessible by any AWS service:\n\n# Log in to AWS ECR\nos.system(f\"aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\")\n\n# Create ECR repo: only needs to be done once\nos.system(f\"aws ecr create-repository --repository-name {ecr_image_name} --image-scanning-configuration scanOnPush=true --image-tag-mutability MUTABLE\")\n\n# Build Docker image using your local Dockerfile\nos.system(f\"docker build -t {ecr_image_name} . --platform=linux/amd64\")\n\n# Tag you image\nos.system(f\"docker tag {ecr_image_name}:latest {account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_image_name}:latest\")\n\n# Push your image to ECR\nos.system(f\"docker push {account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_image_name}:latest\")\n\nLet’s finally create the Lambda function:\n\n# Set up the Lambda client\nlambda_client = boto3.client('lambda', region_name=region)\n\n\n# Function name (not public facing)\nfunction_name = 'lambda-fn-sklearn-predict'\n\n\n# Retrieve (if already exists) or create a new Lambda function\ntry:\n    response = lambda_client.get_function(FunctionName=function_name)\n    print(f\"Lambda function {function_name} already exists\")\nexcept lambda_client.exceptions.ResourceNotFoundException:\n    response = lambda_client.create_function(\n        FunctionName=function_name,\n        Role=role_arn,\n        PackageType='Image',\n        Code={\n            'ImageUri': f'{account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_image_name}:latest'\n        },\n        Description='SKLearn predict Lambda function',\n        Timeout=10,\n        MemorySize=256,\n        Publish=True,\n    )\n    print(f\"Lambda function {function_name} created!\")\n\nLambda function lambda-fn-sklearn-predict created!\n\n\nThe create_function returns instantly but the function will be in the pending state for a few seconds. To be sure, wait 30 seconds before running the invocation command:\n\ntime.sleep(30)\n\n\n# Prepare the event to pass to the Lambda function\nexample = [\"\"\"Did that FAQ ever got modified to re-define strong atheists as not those who \\\nassert the nonexistence of God, but as those who assert that they BELIEVE in \\\nthe nonexistence of God?\"\"\"]\n\n# Invoke the Lambda function\nresponse = lambda_client.invoke(\n    FunctionName=function_name,\n    InvocationType='RequestResponse',\n    Payload=json.dumps({\"body\": example})\n)\n\n# Get the response from the Lambda function\nresult = json.loads(response['Payload'].read())\n\nprint(result[\"body\"])\n\n[\"alt.atheism\"]\n\n\nGreat, now we have a deployed Lambda function that we can use to make predictions! The next step is using API gateway to wrap an endpoint around it."
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#api-gateway-1",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#api-gateway-1",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "API Gateway",
    "text": "API Gateway\nTo create a POST endpoint on API gateway, you will need a few more commands. While the commands below seem overly bureaucratic and hard to remember, consider that you only have to do it once and you can always go to ChatGPT the documentation for help.\n\n# Set up the API Gateway client\napigw_client = boto3.client('apigateway', region_name=region)\n\n\n# Define the name of the API (not public facing)\nrest_api_name = function_name + '-api'\n\n\n# First, lets verify whether we already have an endpoint with this name\nresponse = apigw_client.get_rest_apis()\ncreate_api_gw = True\nfor item in response['items']:\n    if item['name'] == rest_api_name:\n        rest_api_id = item['id']\n        create_api_gw = False\n\n\n# Create the REST API if it doesn't exist\nif create_api_gw:\n    response = apigw_client.create_rest_api(\n        name=rest_api_name,\n        description='API Gateway that triggers a lambda function',\n    )\n    rest_api_id = response['id']\n    \n    # Create a resource\n    response = apigw_client.get_resources(restApiId=rest_api_id)\n    root_id = response['items'][0]['id']\n    response = apigw_client.create_resource(\n        restApiId=rest_api_id,\n        parentId=root_id,\n        pathPart=endpoint,\n    )\n    resource_id = response['id']\n    \n    # Create the POST method\n    response = apigw_client.put_method(\n        restApiId=rest_api_id,\n        resourceId=resource_id,\n        httpMethod='POST',\n        authorizationType='NONE', # WARNING: this will allow public access!\n        apiKeyRequired=True,\n    )\n    \n    # Get the Lambda function ARN\n    lambda_function = lambda_client.get_function(FunctionName=function_name)\n    lambda_arn = lambda_function['Configuration']['FunctionArn']\n    \n    # Set up integration with the Lambda function\n    uri = f\"arn:aws:apigateway:{region}:lambda:path/2015-03-31/functions/{lambda_arn}/invocations\"\n\n    response = apigw_client.put_integration(\n        restApiId=rest_api_id,\n        resourceId=resource_id,\n        httpMethod='POST',\n        type='AWS_PROXY',\n        integrationHttpMethod='POST',\n        uri=uri,\n    )\n\n    # Deploy the API\n    response = apigw_client.create_deployment(\n        restApiId=rest_api_id,\n        stageName=stage,\n    )\n    \n    # Create API key\n    api_key = apigw_client.create_api_key(\n        name=rest_api_name + '-key',\n        description='API key',\n        enabled=True,\n        generateDistinctId=True\n    )\n    \n    # Create usage plan\n    usage_plan = apigw_client.create_usage_plan(\n        name='API usage plan',\n        description='Harsh rate limits and daily quota for public facing API',\n        apiStages=[\n            {\n                'apiId': rest_api_id,\n                'stage': stage,\n            },\n        ],\n        # Very harsh rate limits since this will be public facing\n        throttle={\n            'burstLimit': 10,\n            'rateLimit': 10.0\n        },\n        # Low daily limits for the same reason\n        quota={\n            'limit': 100,\n            'period': 'DAY'\n        }\n    )\n    \n    # Associate the usage plan with the API key\n    response = apigw_client.create_usage_plan_key(\n        usagePlanId=usage_plan['id'],\n        keyId=api_key['id'],\n        keyType='API_KEY'\n    )\n    \n    # Grant API Gateway permission to invoke the Lambda function\n    source_arn = f'arn:aws:execute-api:{region}:{account_id}:{rest_api_id}/*'\n    response = lambda_client.add_permission(\n        FunctionName=function_name,\n        StatementId='apigateway-lambda-invoke-permission',\n        Action='lambda:InvokeFunction',\n        Principal='apigateway.amazonaws.com',\n        SourceArn=source_arn\n    )\n\n\n# The URL by default will follow this pattern:\nurl = f\"https://{rest_api_id}.execute-api.{region}.amazonaws.com/{stage}/{endpoint}/\"\nprint(url)\n\nhttps://9g9npm4j2h.execute-api.eu-west-1.amazonaws.com/test/predict/\n\n\nNow we have an endpoint in production! Anyone, even you, can use it. To have skin in the game, I’m leaving this endpoint open to the public. Try out make a request yourself. Here is how you can do it:\n\n# WARNING: you shouldn't print or commit API keys in general\n# This is for demonstration purposes only\nprint(f\"API key: {api_key['value']}\")\n\nAPI key: Kqrrc4uDk5aFZpH0NLfXW4CvgZphPbrc731nY5Yx\n\n\n\nheaders = {\n    'Content-type': 'application/json', \n    'x-api-key': api_key['value'],\n}  \n\nresp = requests.post(url, headers=headers, json=example)\nresp.json()\n\n['alt.atheism']\n\n\nSuccess! We have an endpoint serving the model at scale without the hassles and costs of running a web server."
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#monitoring",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#monitoring",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "Monitoring",
    "text": "Monitoring\nWe can use AWS CloudWatch to monitor Lambda and API gateway usage, check the logs and create alarms. As a tip from a seasoned AWS engineer, pay special attention to memory usage, execution times and failure rates. Memory and execution time have limits which you can adjust for your particular usecase. But never forget that every resource you use, you pay for it.\n\n\n\nDefault CloudWatch dashboard for Lambda\n\n\nFor ML metrics and issues such as accuracy and covariate shift, we need a different solution. See this repo for some suggestions. With AWS, you will only be able to monitor the “SRE” aspects of your deployment. Don’t ignore that part: the Four Golden Signals are as important to ML as they are to any other web application."
  },
  {
    "objectID": "posts/serverless_model_deployment/serverless_model_deployment.html#acknowledgements",
    "href": "posts/serverless_model_deployment/serverless_model_deployment.html#acknowledgements",
    "title": "Real-Time ML Models with Serverless AWS",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI’d like to give my deepest gratitude to the reviewers of this post: Guilherme Lázari, Raphael Tamaki, Rafael Garcia-Dias, Ramon Maciel and Danilo Pereira. At the time of writing of this post, Guilherme, Ramon and Danilo are all AWS engineers, but this post does not reflect any official position from AWS."
  },
  {
    "objectID": "posts/how_not_to_forecast_election/index.html",
    "href": "posts/how_not_to_forecast_election/index.html",
    "title": "How (not) to forecast an election",
    "section": "",
    "text": "We use a hierarchical Bayesian model to show how a simple pro-Trump bias has a huge effect on forecasting election results. See the discussion on HackerNews.\n\nUS presidential elections\nUnlike other countries, where presidential elections follow a simple majority rule, the American election follows a different set of rules that makes it very hard for outsiders to understand what is going on. However, the idea is quite simple: each state has a number of electoral college votes, totaling 538 votes across the nation. The winner candidate of each state election takes all of its electoral college votes (with two exceptions). If a candidate receives 270 electoral votes or more overall, he or she wins the election. This makes the election an interesting forecasting problem. Instead of predicting the total number of votes in each candidate, the problem is forecasting the results of each state individually.\nOur interest is not to forecast the 2016 election, which is not a random variable anymore (spoiler alert: Trump won). We want to show how a Bayesian forecasting model works, and propose a modification that explains why statistical models would fail so badly if there were polling biases.\n\n\nPolling\nIn theory, a poll should randomly sample the voting population. However, in practice there are several possible polling problems that can compromise their use in naive statistical analyses:\n\nShy Tory and Bradley effects: Voters are not comfortable with revealing their particular preference.\nSelection bias: People who answer the pollsters are from an unrepresentative subgroup (e.g. people who answer landline phones).\nStratification errors: Wrong subpopulation determination when doing stratified sampling.\nCandidate and (non)-voting correlation: As voting is not mandatory, the presidential preference of some people may be correlated with their chance of (not) voting.\nTemporal preference change: The preferences change over time (a lot of people make up their minds in the last week).\nSample noise: Any sample from a population will be noisy and its statistics will not be identical to the population statistics.\n\nSimple forecasts usually only consider the last item when estimating the margins of error or uncertainty. If you only consider this and nothing more, multiple polls will be treated as independent and unbiased, and the more polls you use, the smaller the forecasting errors will be, until there is almost certainty. The other effects are biases that will make the average of multiple polls unreliable and possibly useless. As they are not directly available for modeling, the only way to estimate them is by making strong assumptions or using polling results from previous elections. We do not attempt to model exactly those issues here, we rather include all of them in a bias term that is shared across all polls, and show that even a small bias term favorable to Trump completely changes the forecast.\n\n\nForecast\nWe leave the details of our hierarchical Bayesian forecasting model at the end, for the advanced reader. Now we show the results of its forecasts. However, depending on which polls we include, we have very different results. The polls to use in a forecasting model should be recent and of good quality (conducted with appropriate methodology and with past successes). As a proxy of quality, we use the grades from 538.\nWe found that the simple choice of which polls to use has a very large impact on the probability of victory of each candidate:\n\n\n\n\n\n\n\n\n\nPolling choice\nClinton\nTrump\nNeither\n\n\n\n\nLast poll with good grade\n87.3%\n10.5%\n2.2%\n\n\nPolls with the best 3 grades (over the last month)\n99.3%\n0.6%\n0.1%\n\n\nAll polls from last week\n100.0%\n0.0%\n0.0%\n\n\n\nWe consider polls with good grades to be above B, but this is not possible for some states, so we use a more tolerant definition in those cases. When we say last week or last month, we mean from the Election Day (Nov 8, 2016).\nBy aggregating the polls with different weights (e.g. according to their quality or distance from election), we would have endless forecasting possibilities, which explains the diversity found in the media before the election:\n\nNew York Times: 85% Clinton, 15% Trump\nHuffington Post: 98% Clinton, 1.7% Trump\nPrinceton Election Consortium: 99% Clinton\n538: 71.4% Clinton, 28.6% Trump\n\nOf the mainstream forecasters, only 538 cannot be included in the bucket of certain Clinton victory. Notice that the other forecasts are consistent with our own results shown in the previous table. How come the forecasters, including us, made such egregious mistakes? As in many statistical problems, the answer lies with the data: garbage in, garbage out.\n\n\nBias impact\nWe encompass all possible polling issues in a general bias term. We use a bias term that is favorable to Trump on election day because that is clearly what happened on November 8. This can be interpreted as a hidden preference for Trump that is not captured by the polls by all the issues explained before. Instead of fixing the bias to an arbitrary value, we use a uniform random variable. We start with zero bias, where Clinton is almost surely the victor, and increase its span until Trump is almost certainly the victor:\n\n\n\n\n\nWe see that even a small polling bias has a huge effect on the election forecast. This explains why 538 had a more favorable Trump forecast as they included a polling bias and did not treat the polls as independent samples, but this also indicates that even 538 probably underestimated the polling biases.\nYou can check for yourself how the bias impacts the results of each state election on the map below. For each state we forecast the predicted percentage of votes of each candidate, with varying bias:\n\n\n\nHierarchical Bayesian forecasting model\nWe can use a hierarchical Bayesian model to aggregate the information of each state poll to form a globally coherent forecast. Overall, each poll’s likelihood is modeled as a multinomial, with Dirichlet prior (per state) and uniform hyperprior (per nation). This way, prior information is shared across states and we can use weakly informative hyperpriors. We start with an overall national preference over each candidate, modeled as three independent wide uniform distributions:\n\n\n\nThen, we have the voting intention in each state, with the national preference as prior:\n\n\n\nFinally, each poll is modeled as one independent sample of the voting intention of the state:\n\n\n\nWe infer the posteriors of the unknown parameters (state voting intentions and national preferences) given the observed variables (the polls we decided to include). The posterior is our knowledge of the unseen variables after observing statistically related variables. Depending on the choice of which polls to include as observations, as we explained before, the posteriors and thus the forecast will be different.\n\n\n\nTo forecast the probability of each candidate winning the state election, we use the same multinomial likelihood that was used for the inference. However, now the voting intentions are the posterior given the polls, and number of voters is chosen to match 2012 election numbers. Thus, for each state we sample the predicted number of votes of each candidate on election day using the following formula:\n\n\n\nThe candidate with more votes takes all the electoral colleges of the state (we ignore the particularities of Nebraska and Maine). We sum the electoral colleges votes of each candidates, and if a candidate wins 270 votes or more, he or she is the winner. We repeat this process multiple times in order to determine the probability of each candidate winning the election.\nTo add the bias term in our forecast to account for all the polling issues already cited, we make a simple change to the predictive model:\n\n\n\nThis bias always stochastically favors Trump. We must subtract the same value from Clinton in order to guarantee θbias remains a valid probability simplex. In our experiments above, we vary ϵ from 0 to 5%.\n\n\nStan Code\nTake a look at our code and feel free to play with it. Here is how we implemented our model in Stan:\ndata {\n    int nb_polls; // Number of polls\n    int nb_states; // Number of states (51 because of D.C.)\n    int nb_candidates; // Number of candidates (3: Trump, Clinton, Ind.)\n    int polls_states[nb_polls]; // Poll -&gt; state map\n    int votes[nb_polls, nb_candidates]; // Polled votes for each candidate\n    int nb_voters[nb_states]; // Number of voters for forecasting\n    real bias; // Polling bias\n}\nparameters {\n    simplex[nb_candidates] theta[nb_states]; //1 - Trump, 2 - Clinton, 3 - Ind.\n    vector[nb_candidates] alpha;\n}\nmodel {\n    for(c in 1:nb_candidates)\n        alpha[c] ~ uniform(0, 1000); // Weakly informative hyperprior\n\n    for(s in 1:nb_states)\n        theta[s] ~ dirichlet(alpha); // Dirichlet prior per state\n\n    for(p in 1:nb_polls) // Multinomial observations (polled values)\n        votes[p] ~ multinomial(theta[polls_states[p]]);\n}\ngenerated quantities {\n    int votes_pred[nb_states, nb_candidates]; // Predicted number of votes on election day\n    real epsilon[nb_states]; // Bias random variable\n    simplex[nb_candidates] theta_bias[nb_states]; // Biased voting intentions\n\n    // The deltas below are used to ensure that the biased thetas form a valid simplex\n    real delta_t[nb_states];\n    real delta_h[nb_states];\n    real delta[nb_states];\n\n    for(s in 1:nb_states) {\n        if(bias == 0.0)\n            epsilon[s] &lt;- 0.0;\n        else\n            epsilon[s] &lt;- uniform_rng(0, bias); // Bias value for this state\n\n        // We must ensure that theta will remain a valid probability simplex,\n        // so we limit delta in a way theta will never be below 0 or above 1\n        delta_t[s] &lt;- fabs(theta[s][1] - fmax(0.0, fmin(1.0, theta[s][1] + epsilon[s])));\n        delta_h[s] &lt;- fabs(theta[s][2] - fmin(1.0, fmax(0.0, theta[s][2] - epsilon[s])));\n        delta[s] &lt;- fmin(delta_t[s], delta_h[s]);\n\n        theta_bias[s][1] &lt;- theta[s][1] + delta[s];\n        theta_bias[s][2] &lt;- theta[s][2] - delta[s];\n        theta_bias[s][3] &lt;- theta[s][3];\n\n        votes_pred[s] &lt;- multinomial_rng(theta_bias[s], nb_voters[s]);\n    }\n}\n\n\nConclusion\nWe have found that even a modestly sophisticated statistical model does very little to counter unreliable data. A proper forecasting model for the American elections must include polling biases, as we have shown in a general way. To arrive at a precise number, you must either make assumptions on the polling methodology, or calibrate the polls weights using their historical reliability. This could be the reason that 538 had Trump winning at the highest probability of the mainstream media. We have also to consider that when you forecast 70% probability of winning, the prediction is expected to fail 30% of the time, so it is hard to evaluate models and forecasters using only one observation.\nEven though the 2016 election was one of most surprising polling misses in recent years, the result was not a black swan. Nassim Nicholas Taleb pointed out, before the election, that the mainstream forecasts were not reliable due to their significant volatility. According to his model based on option theory, this volatility should have pulled the probabilities toward 50-50. As a follow-up, if there is interest, we want to explore a time-series model where the voting intentions follow a random walk. To do this, we need to change the underlying model to allow the unseen random walk influence the polling results. Following Andrew Gelman’s suggestion, we can change the Dirichlet prior to a softmax prior, and then we can make the softmax parameters follow a random walk.\n\nAcknowledgements\nThis blog post was written with the help of Ramon Oliveira back when we were working in a data science consulting company (Datart) that we co-founded together."
  },
  {
    "objectID": "posts/name_classification/name_classification.html",
    "href": "posts/name_classification/name_classification.html",
    "title": "Name classification with ChatGPT",
    "section": "",
    "text": "I explore the problem of name classification with ChatGPT and three machine learning models of increasing complexity: from logistic regression to FastAI LSTM to Hugging Face transformer. To see all the code and reproduce the results, check out the notebook."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#name-classification",
    "href": "posts/name_classification/name_classification.html#name-classification",
    "title": "Name classification with ChatGPT",
    "section": "Name classification",
    "text": "Name classification\nCan you classify a name as belonging to a person or company? Some are easy, like “Google” is a company and “Pedro Tabacof” is a name. Some are trickier, like “John Deere”. With a labelled dataset, we can train a machine learning model to classify names into entities. This is a simplification of the more general task called Named Entity Recognition. This can also be seen as a simple version of document classification, where the document is simply a name. Due to its simplicity and relation to typical NLP problems, name classification is a good candidate to experiment with different NLP technologies.\nWhen I heard a friend was working on a name classification problem as part of a hiring process, I went straight to ChatGPT to look for answers. I soon realised that ChatGPT can do a great job itself classifying names into entities with just a couple of examples (one-shot learning):\n\nNow, if I actually productionize that prompt using ChatGPT’s API, how would it compare to more traditional alternatives? In NLP, traditional might mean a model from just 5 years ago!\nIn this post, I explore four ways to classify names into person or company:\n\nBaseline using word counts and logistic regression: typical baseline for text classification\nFastAI LSTM fine-tuning (whole network): simple fine-tuning with few lines of code\nHuggingface DistilBERT fine-tuning (head only): more involved neural network training using PyTorch\nChatGPT API one-shot learning: only prompt engineering and post-processing are needed\n\nI use two public datasets available on Kaggle: IMDb Dataset for people names and 7+ Million Company Dataset for companies. Those datasets are large, with almost 20 million names! The choice of datasets was inspired by the open-source business individual classifier by Matthew Jones, which achieves 95% accuracy on this name classification task.\nFor simplicity, I sample 1M names for training and 100k for testing with a 50-50 balance between companies and people. Since we have balanced classes and ChatGPT cannot produce scores or probabilities (so we cannot use ROC AUC or average precision, definitely a big limitation of ChatGPT), I decided to use the accuracy as the main metric."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#datasets",
    "href": "posts/name_classification/name_classification.html#datasets",
    "title": "Name classification with ChatGPT",
    "section": "Datasets",
    "text": "Datasets\nFirst, I will download the datasets from Kaggle and do some basic preprocessing. To reproduce the results, you will need a Kaggle account and its command line installed locally. You need to add your API key and username to a file kaggle.json, which is found in the directory defined by the environment variable called KAGGLE_CONFIG_DIR.\n\n\nDownloading free-7-million-company-dataset.zip to /notebooks\n 95%|██████████████████████████████████████  | 265M/278M [00:03&lt;00:00, 93.1MB/s]\n100%|████████████████████████████████████████| 278M/278M [00:03&lt;00:00, 80.1MB/s]\n\n\n\n\nDownloading imdb-dataset.zip to /notebooks\n 99%|██████████████████████████████████████▋| 1.05G/1.06G [00:06&lt;00:00, 153MB/s]\n100%|███████████████████████████████████████| 1.06G/1.06G [00:06&lt;00:00, 163MB/s]\n\n\n\n\nArchive:  free-7-million-company-dataset.zip\n  inflating: companies_sorted.csv    \n\n\n\n\nArchive:  imdb-dataset.zip\n  inflating: data.tsv                \n\n\nI do some preprocessing, inspired by the open-source repo I got the datasets inspiration from: 1. Lower case the people dataset since the companies dataset is all lower case (otherwise I’d suggest keeping the original case, as that can be informative). 2. Remove odd characters and unnecessary spaces. 3. Remove empty and null rows.\n\ncompanies = pd.read_csv(\"companies_sorted.csv\", usecols=[\"name\"])\n\npeople = (\n    pd.read_csv(\"data.tsv\", sep=\"\\t\", usecols=[\"primaryName\"])\n    # Since the companies are all lower case, we do the same here to be fair\n    .assign(name=lambda df: df.primaryName.str.lower()).drop(\"primaryName\", axis=1)\n)\n\ndf = pd.concat(\n    (companies.assign(label=\"company\"), people.assign(label=\"person\"))\n).sample(frac=1.0, random_state=42)\n\ninvalid_letters_pattern = r\"\"\"[^a-z0-9\\s\\'\\-\\.\\&]\"\"\"\nmultiple_spaces_pattern = r\"\"\"\\s+\"\"\"\n\ndf[\"clean_name\"] = (\n    df.name.str.lower()\n    .str.replace(invalid_letters_pattern, \" \", regex=True)\n    .str.replace(multiple_spaces_pattern, \" \", regex=True)\n    .str.strip()\n)\n\ndf = df[\n    ~df.clean_name.isin([\"\", \"nan\", \"null\"]) & ~df.clean_name.isna() & ~df.label.isna()\n][[\"clean_name\", \"label\"]]\n\ndf.head(10)\n\n\n\n\n\n\n\n\nname\nlabel\n\n\n\n\n10103038\njinjin wang\nperson\n\n\n5566324\nnative waterscapes, inc.\ncompany\n\n\n8387911\njeff killian\nperson\n\n\n6783284\nlisa mareck\nperson\n\n\n9824680\npablo sánchez\nperson\n\n\n6051614\ndvc sales\ncompany\n\n\n6479728\norso balla\nperson\n\n\n4014268\ntwo by three media\ncompany\n\n\n2093936\nhouse of light and design\ncompany\n\n\n11914237\nhamdy faried\nperson\n\n\n\n\n\n\n\nFrom the value counts below, we can see that we have 19.5 million names, 63% being people and 37% companies.\n\ndf.label.value_counts()\n\nperson     12344506\ncompany     7173422\nName: label, dtype: int64\n\n\nI sample 550k people and companies to make the dataset balanced and then split into 1M training and 100k testing examples.\n\ntrain_df = pd.concat(\n    (\n        df[df.label == \"company\"].sample(n=1_100_000 // 2),\n        df[df.label == \"person\"].sample(n=1_100_000 // 2),\n    )\n)\n\ntrain_df, test_df = train_test_split(train_df, test_size=100_000, random_state=42)\n\nI save the processed datasets for easier iteration. Tip: If you have large datasets, always try to save your preprocessed datasets to disk to prevent wasted computation.\n\n# Saving the processed dataframes locally for quicker iterations\ntrain_df.to_csv(\"train_df.csv\", index=False)\ntest_df.to_csv(\"test_df.csv\", index=False)\n\n# Freeing up the memory used by the dataframes\ndel companies, people, df, train_df, test_df\ngc.collect()\n\nSince I freed up the memory of all datasets, I need to reload them:\n\n# Just run from here if the datasets already exist locally\ntrain_df = pd.read_csv(\"train_df.csv\")\ntest_df = pd.read_csv(\"test_df.csv\")\n\ntrain_df.shape, test_df.shape\n\n((1000000, 2), (100000, 2))\n\n\nNow, I have one single dataset for training with 500k people and 500k companies and one single test set with 50k people and 50k companies."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#exploratory-data-analysis",
    "href": "posts/name_classification/name_classification.html#exploratory-data-analysis",
    "title": "Name classification with ChatGPT",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nBefore I actually get to the fun part, let’s understand the data we have first. I have two hypotheses to explore:\n\nDo we see a different distribution of words per class? I’d expect some words like “ltd” to be present only in companies and words like “john” to be over-represented in names.\nDoes sentence length vary by class? I expect higher range for companies than people, as companies can be from just two characters like “EY” to mouthfuls like “National Railroad Passenger Corporation, Amtrak”. Alternatively, I could look at the number of words per sentence, since most Western names are around 3 words.\n\nAnyway, beware the Falsehoods Programmers Believe About Names.\n\nwords_df = (\n    train_df.assign(word=train_df.clean_name.str.split(\" +\"))\n    .explode(\"word\")\n    .groupby([\"word\", \"label\"])\n    .agg(count=(\"clean_name\", \"count\"))\n    .reset_index()\n)\n\ntotal_words = words_df[\"count\"].sum()\n\nwords_df = words_df.assign(freq=words_df[\"count\"]/total_words)\n\nperson_words = (\n    words_df[words_df.label == \"person\"].sort_values(\"freq\", ascending=False).head(25)\n)\ncompany_words = (\n    words_df[words_df.label == \"company\"].sort_values(\"freq\", ascending=False).head(25)\n)\n\nFirst, let’s take a look at the word counts by label:\n\n\n\n                                                \n\n\n\n\n\n                                                \n\n\nWe can see our hypothesis was right: Some words are quite predictive of being a person or company name. Note that there is no intersection between the top 25 words for people and companies. This insight implies a simple but effective baseline would be a model built on top of word count, which is what I do next. However, there is a long tail of possible names, so we have to go beyond the most common ones. Another way to see how the distributions differ is by sentence length:\n\n\n\n                                                \n\n\nCompany names tend to be longer on average and have a higher variance, but interestingly they both peak at 13 characters. I could use sentence length as a feature, but let’s stick to word counts for now."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#baseline-word-counts-logistic-regression",
    "href": "posts/name_classification/name_classification.html#baseline-word-counts-logistic-regression",
    "title": "Name classification with ChatGPT",
    "section": "Baseline: Word counts + Logistic regression",
    "text": "Baseline: Word counts + Logistic regression\nLet’s start with a simple and traditional NLP baseline: word frequency and logistic regression. Alternatively, we could use Naive Bayes, but I prefer logistic regression for its greater generality and easier interpretation as a linear model.\nTypically, we use TF-IDF instead of word counting for document classification. Since names are quite short and repetitive words (e.g. “John”) are predictive, I believe it not to be useful here. Indeed, a quick test showed no improvement to accuracy by using TF-IDF.\nAnother varation is to use n-grams for either words or characters: they’re left as a suggestion to the reader.\n\ntext_transformer = CountVectorizer(analyzer=\"word\", max_features=10000)\nX_train = text_transformer.fit_transform(train_df[\"clean_name\"])\nX_test = text_transformer.transform(test_df[\"clean_name\"])\n\nlogreg = LogisticRegression(C=0.1, max_iter=1000).fit(\n    X_train, train_df.label == \"person\"\n)\npreds = logreg.predict(X_test)\n\nbaseline_accuracy = accuracy_score(test_df.label == \"person\", preds)\nprint(f\"Baseline accuracy is {round(100*baseline_accuracy, 2)}%\")\n\nBaseline accuracy is 89.49%\n\n\n89.5% accuracy is not bad for a linear model! Remember, since the datasets are balanced, a baseline accuracy without any information would be 50%. Now, whether this is good or bad in an absolute sense, it depends on the actual application of the model. It also depends on the distribution of the words this model would actually see in production. The datasets I used are quite general, containing all kinds of people and company names. In a real application, the names could be more constrained (e.g. only coming from a particular country).\nNow, let’s see what mistakes the model makes (error analysis). It’s always interesting to look at examples where the model makes the worst mistakes. If we have a tabular dataset, it might be difficult to interpret what is going on, but, for perceptual data a human can understand (text, image, sound), this leads to invaluable insights into the model.\n\ntest_df[\"proba_person\"] = logreg.predict_proba(X_test)[:, 1]\ntest_df[\"abs_error\"] = np.where(\n    test_df.label == \"person\", 1 - test_df.proba_person, test_df.proba_person\n)\n\ntest_df.sort_values(\"abs_error\", ascending=False)[\n    [\"clean_name\", \"label\", \"proba_person\"]\n].head(10)\n\n\n\n\n\n\n\n\nclean_name\nlabel\nproba_person\n\n\n\n\n60581\nco co mangina\nperson\n0.000206\n\n\n49398\nbuster benton and the sons of blues\nperson\n0.000984\n\n\n6192\nbest horizon consulting\nperson\n0.001613\n\n\n83883\nles enfants du centre de loisirs de chevreuse\nperson\n0.002633\n\n\n84646\nmanuel antonio nieto castro\ncompany\n0.997350\n\n\n32669\nchris joseph\ncompany\n0.996298\n\n\n8545\nhub kapp and the wheels\nperson\n0.004568\n\n\n77512\nmichael simon p.a.\ncompany\n0.994109\n\n\n71392\ndylan ryan teleservices\ncompany\n0.993017\n\n\n64777\nnetherlands national field hockey team\nperson\n0.007220\n\n\n\n\n\n\n\nWe can see that the mistakes are mostly understandable: There are many companies named just like people. How could the model know Chris Joseph is a company and not a person? The only way would be with information not available in the data I provided for its learning. We also see mislabelings in the people dataset: “netherlands national field hockey team” and “best horizon consulting” do not sound like people names!\nThis implies a high-leverage activity here would be cleaning the people dataset. If you want to make the data cleaning process sound sexier, just call it data-centric AI (just kidding: data-centric AI is actually a good framework to use for real-life machine learning applications where, in almost all cases, data trumps modelling)."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#fastai-lstm-fine-tuning",
    "href": "posts/name_classification/name_classification.html#fastai-lstm-fine-tuning",
    "title": "Name classification with ChatGPT",
    "section": "FastAI LSTM fine tuning",
    "text": "FastAI LSTM fine tuning\nFor the first more complex machine learning model, let’s start with FastAI due to is simple interface. Following the suggestion of this article, I used an AWD_LSTM model which was pre-trained as a language model that predicts the next word using Wikipedia as dataset. Then, I fine-tune the model with our classification problem. FastAI fine-tune works in the following way: in the first epoch, it only trains the head (the newly inserted neural network on top of the pre-trained language model), then, for all subsequent epochs, it trains the whole model together. FastAI uses many tricks to make the training more effective, which is all wrapped in a simple function call. While convenient, it makes understanding what is going on behind the scenes and any customization more difficult.\n\nfastai_df = pd.concat((train_df.assign(valid=False), test_df.assign(valid=True)))\ndls = TextDataLoaders.from_df(\n    fastai_df, text_col=\"clean_name\", label_col=\"label\", valid_col=\"valid\"\n)\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(5, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.513145\n0.397019\n0.802810\n03:37\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.167889\n0.137422\n0.952030\n07:53\n\n\n1\n0.157485\n0.145000\n0.956200\n07:50\n\n\n2\n0.122625\n0.139295\n0.963160\n07:53\n\n\n3\n0.112604\n0.112886\n0.968730\n07:53\n\n\n4\n0.111916\n0.111421\n0.970460\n07:53\n\n\n\n\n\nNow we ended with 97.1% accuracy, almost 8 percentage points higher than our baseline! Not bad for a few lines of code and one hour of GPU time. Can we do better? Let’s try using a 🤗 transformer."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#hugging-face-distilbert-classification-head-training",
    "href": "posts/name_classification/name_classification.html#hugging-face-distilbert-classification-head-training",
    "title": "Name classification with ChatGPT",
    "section": "Hugging Face DistilBERT classification head training",
    "text": "Hugging Face DistilBERT classification head training\nHugging Face offers hundreds of possible deep learning models for inference and fine-tuning. I chose DistilBERT due to time and GPU memory constraints. By default, Hugging Face trainer will fine-tune all the weights of the model, but now I just want to train the classification head, which is a two-layer fully-connected neural network (aka MLP). The reason is twofold: 1. We’re dealing with a simple problem and 2. I don’t want to leave the model training for too long to make reproducibility simpler and reduce GPU costs. I worked backwards from the previous results: Since FastAI took roughly one hour, I wanted to use the same GPU time budget here.\nTo only train the classification head, I had to use the PyTorch interface, which allows for more flexibility. First, I download DistilBERTs tokenizer, apply it to our dataset, then download the model itself, mark all layers as requiring no gradient (i.e. not trainable), and then train the classification head.\n\nbatch_size = 32\nnum_epochs = 3\nlearning_rate = 3e-5\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\nFirst, I apply the DistilBERT tokenizer to our datasets:\n\ntokenized_train_df = tokenizer(\n    text=train_df[\"clean_name\"].tolist(), padding=True, truncation=True\n)\ntokenized_test_df = tokenizer(\n    text=test_df[\"clean_name\"].tolist(), padding=True, truncation=True\n)\n\nNow, I create a PyTorch dataset that is the able to hand the input format given by the tokenizer (tokens + attention mask), alongside the labels:\n\ntrain_dataset = NamesDataset(\n    tokenized_train_df, (train_df.label == \"person\").astype(int)\n)\ntest_dataset = NamesDataset(tokenized_test_df, (test_df.label == \"person\").astype(int))\n\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2\n)\n\nI set all the base model parameters as non-trainable (so that only the classification head is trained):\n\nfor param in model.distilbert.parameters():\n    param.requires_grad = False\n\nFinally, I actually train the model (see full training and evaluation code in the notebook):\n\n\n\n\n\n\n\n\nepoch 0: test accuracy is 96.527%\nepoch 1: test accuracy is 96.776%\nepoch 2: test accuracy is 96.854%\n\n\n\n\n\n\n\n\nWe got 96.8% accuracy, essentially the same as FastAI LSTM model. This implies the extra complexity here was for nought. Of course, this problem is a simple one: If I had a more complex problem, I’m sure using a stronger pre-trained language model would give an edge relative to the simpler LSTM trained on Wikipedia. Also, by not fine-tuning the whole network, we miss out on the full power of the transformer. But this suggests that you shouldn’t write off FastAI without trying, which, as I show above, is quite simple.\nLet’s see which mistakes this model is making:\n\ntest_df[\"proba_person\"] = test_preds\ntest_df[\"abs_error\"] = np.where(\n    test_df.label == \"person\", 1 - test_df.proba_person, test_df.proba_person\n)\ntest_df.sort_values(\"abs_error\", ascending=False)[\n    [\"clean_name\", \"label\", \"proba_person\"]\n].head(10)\n\n\n\n\n\n\n\n\nclean_name\nlabel\nproba_person\n\n\n\n\n6192\nbest horizon consulting\nperson\n0.000008\n\n\n47006\nrolf schneebiegl & seine original schwarzwald-musi\nperson\n0.000326\n\n\n58512\ndevelopment\nperson\n0.000363\n\n\n9404\nxin yuan yao\ncompany\n0.999556\n\n\n59585\ncheng hsong\ncompany\n0.999490\n\n\n46757\ncompagnie lyonnaise du cin ma\nperson\n0.000550\n\n\n38224\npawel siwczak\ncompany\n0.999389\n\n\n25983\nsarah hussain\ncompany\n0.999311\n\n\n23870\nmanjeet singh\ncompany\n0.999295\n\n\n73909\nglassworks\nperson\n0.000776\n\n\n\n\n\n\n\nAgain, we see cases of clear mislabeling in the case of person and some tough cases in the case of company. Given the accuracy and the worst mistakes, we may be at the limit of what can be done for this dataset without cleaning it. Now, the final question: Can I get the same level of accuracy without any supervised training at all?"
  },
  {
    "objectID": "posts/name_classification/name_classification.html#chatgpt-api-one-shot-learning",
    "href": "posts/name_classification/name_classification.html#chatgpt-api-one-shot-learning",
    "title": "Name classification with ChatGPT",
    "section": "ChatGPT API one-shot learning",
    "text": "ChatGPT API one-shot learning\nI will use OpenAI’s API to ask ChatGPT to do name classification for us. First, I need to define the prompt very carefully, what is now called prompt engineering. There are some rules of thumb for prompt engineering. For example, always give concrete examples before asking ChatGPT to generalize to new ones.\nThe ChatGPT API has three prompt types:\n\nSystem: Helps set the tone of the conversation and gives overall directions\nUser: Represents yourself, use it to state your task or need\nAssistant: Represents ChatGPT, use it to give examples of valid or reasonable responses\n\nYou can mix and match all prompt types, but I suggest starting with the system one, having at least one round of task-response examples, then restating the task that will actually be completed by ChatGPT.\nHere, I ask for ChatGPT to classify 10 names into person or company. If I ask for more, say 100 names, there is a higher chance of failure (e.g. it sees a weird string and complains there is nothing it can do regarding the whole batch). If there is still a failure, I do a backup query on each name individually. If ChatGPT fails to provide a clear answer on an individual name, I default to answering “company” since this class contains more problematic strings.\nFinally, how can I extract the labels from ChatGPT’s response? It might answer differently, for example, by fixing a misspelling or by using uppercase instead of lowercase (system prompt notwithstanding). In general, it answers in the same order, but can I rely on that completely for all 100k examples? To be safe, I do a simple string matching based on the Levenshtein distance to match the names I query with ChatGPT’s responses.\nTo reproduce the code below, you need to have an OpenAI account and OPENAI_API_KEY set in your environment\n\nsystem_prompt = \"\"\"\nYou are a named entity recognition expert.\nYou only answer in lowercase.\nYou only classify names as \"company\" or \"person\".\n\"\"\"\n\ntask_prompt = \"Classify the following names into company or person:\"\n\nexamples_prompt = \"\"\"google: company\njohn smith: person\nopenai: company\npedro tabacof: person\"\"\"\n\nbase_prompt = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": task_prompt},\n    {\"role\": \"assistant\", \"content\": examples_prompt},\n    {\"role\": \"user\", \"content\": task_prompt},\n]\n\nall_preds = []\n\n\ndef get_chatgpt_preds(batch_df):\n    \"\"\" Gets predictions for a whole batch of names using ChatGPT's API\"\"\"\n    prompt = base_prompt.copy()\n    prompt += [{\"role\": \"user\", \"content\": \"\\n\".join(batch_df.clean_name)}]\n\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    try:\n        # Max tokens as 20000 is enough in practice for 10 names plus the prompt\n        # Temperature is set to 0 to reduce ChatGPT's \"creativity\"\n        # Model `gpt-3.5-turbo` is the latest ChatGPT model, which is 10x cheaper than GPT3\n        response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\", messages=prompt, max_tokens=2000, temperature=0\n        )\n\n        # Since we gave examples as \"name: class\", ChatGPT almost always follows this pattern in its answers\n        text_result = response[\"choices\"][0][\"message\"][\"content\"]\n        clean_text = [\n            line.lower().split(\":\") for line in text_result.split(\"\\n\") if \":\" in line\n        ]\n\n        # Fallback query: if I cannot find enough names on the response, I ask for each name separately\n        # Without it, we'd have parsing failures once every 10 or 20 batches\n        if len(clean_text) &lt; len(batch_df):\n            clean_text = []\n            for _, row in batch_df.iterrows():\n                prompt = base_prompt.copy()\n                prompt += [{\"role\": \"user\", \"content\": row.clean_name}]\n\n                response = openai.ChatCompletion.create(\n                    model=\"gpt-3.5-turbo\", messages=prompt, max_tokens=2000, temperature=0\n                )\n\n                row_response = response[\"choices\"][0][\"message\"][\"content\"]\n                if \":\" in row_response:\n                    clean_text.append(\n                        [row_response.split(\":\")[0], row_response.split(\":\")[-1]]\n                    )\n                else:\n                    clean_text.append([row.clean_name, \"company\"])  # defaults to company\n\n        # To ensure I'm matching the query and the corresponding answers correctly,\n        # I find the closest sentences in the Levenshtein distance sense\n        batch_df = batch_df.copy()\n        batch_df = batch_df.merge(pd.DataFrame({\"resp\": clean_text}), how=\"cross\")\n        batch_df[\"resp_name\"] = batch_df.resp.str[0].str.strip()\n        batch_df[\"resp_pred\"] = batch_df.resp.str[-1].str.strip()\n        batch_df[\"dist\"] = batch_df.apply(\n            lambda row: lev.distance(row.clean_name, row.resp_name), axis=1\n        )\n        batch_df[\"rank\"] = batch_df.groupby(\"clean_name\")[\"dist\"].rank(\n            method=\"first\", ascending=True\n        )\n        batch_df = batch_df.query(\"rank==1.0\")[[\"clean_name\", \"label\", \"resp_pred\"]]\n        \n    # Catches all errors\n    # Errors only arise due to failures from OpenAI API and should be quite rare\n    # Ideally, we should keep retrying with exponential backoff\n    except Exception as e:\n        print(\"Exception:\", str(e))\n        batch_df = batch_df.copy()\n        batch_df[\"resp_pred\"] = \"company\" # defaults to company\n\n    return batch_df\n\n\nchatgpt_num_workers = 32\nchatgpt_batch_size = 10\nsplit_size = len(test_df) // chatgpt_batch_size\ntest_batches = np.array_split(test_df, split_size)\n\nchatgpt_preds = Parallel(n_jobs=chatgpt_num_workers, verbose=5)(\n    delayed(get_chatgpt_preds)(batch_df) for batch_df in test_batches\n)\n\n\nchatgpt_preds = pd.concat(chatgpt_preds)\nchatgpt_accuracy = (chatgpt_preds.resp_pred == chatgpt_preds.label).sum() / len(\n    chatgpt_preds\n)\n\nprint(f\"ChatGPT accuracy is {round(100*chatgpt_accuracy, 2)}%\")\n\nChatGPT accuracy is 97.52%\n\n\nIncredible! With 97.5% accuracy, ChatGPT managed to outperform complex neural networks trained for this specific task. One explanation is that it used its knowledge of the world to understand some corner cases that the models could not have possibly learned from the training set alone. In some sense, this would be a form of “leakage”: perhaps ChatGPT would be weaker classifying companies founded after its cutoff date (2021).\nChatGPT is also quite cheap to run: the cost to classify the 100k examples was just under $5. It took 18 min to score all the examples, which could probably be improved by better parallelism. If you don’t use any parallelism at all, it will be much slower.\nLet’s see the raw responses ChatGPT gives:\n\nchatgpt_preds.resp_pred.value_counts().head(20)\n\nperson                                                                                                                            50955\ncompany                                                                                                                           48913\ncompany or person (not enough context to determine)                                                                                  13\nit is not clear whether it is a company or a person.                                                                                 11\nneither (not a name)                                                                                                                  7\nnot a name                                                                                                                            6\nit is not clear whether this is a company or a person.                                                                                5\ncannot be classified as either company or person                                                                                      4\ncompany or person (not enough information to determine)                                                                               4\ni'm sorry, i cannot classify this name as it does not appear to be a valid name.                                                      3\nneither                                                                                                                               3\nit is not clear whether it is a person or a company.                                                                                  2\nn/a (not a name)                                                                                                                      2\ni am sorry, i cannot classify this name as it does not provide enough information to determine if it is a company or a person.        2\nthis is not a name.                                                                                                                   2\nthis is not a valid name.                                                                                                             2\nperson (assuming it's a misspelling of a person's name)                                                                               2\nneither person nor company (not a name)                                                                                               2\nperson or company (without more context it is difficult to determine)                                                                 2\nplace                                                                                                                                 2\nName: resp_pred, dtype: int64\n\n\nFor the vast majority of cases, ChatGPT answers as I request: person or company. In very rare cases, it states it doesn’t know, it’s not clear or could be both. What are such examples in practice?\n\nchatgpt_preds[~chatgpt_preds.resp_pred.isin([\"person\", \"company\"])][[\"clean_name\", \"label\", \"resp_pred\"]].head(10)\n\n\n\n\n\n\n\n\nclean_name\nlabel\nresp_pred\n\n\n\n\n55\nalkj rskolen ringk bing\ncompany\nneither (not a valid name)\n\n\n55\n81355\nperson\ncannot be classified without more context\n\n\n22\ntelepathic teddy bear\nperson\nneither\n\n\n11\nagebim\ncompany\nit is not clear whether it is a company or a person.\n\n\n44\ni quit smoking\ncompany\nneither company nor person\n\n\n33\nsaint peters church\ncompany\ncompany (assuming it's a church organization)\n\n\n88\nholy trinity lutheran church akron oh\ncompany\ncompany (assuming it's a church organization)\n\n\n55\ndisplayname\ncompany\ncompany or person (not enough context to determine)\n\n\n66\nken katzen fine art\ncompany\ncompany or person (not enough context to determine)\n\n\n66\ncolumbus high school\ncompany\ncompany (assuming it's a school)\n\n\n\n\n\n\n\nThe names ChatGPT cannot classify are definitely tricky, like “81355” or “telepathic teddy bear”. In some cases, like for “saint peters church”, it does get it right with some extra commentary in parenthesis. All in all, I’d say ChatGPT did an amazing job and failed in a very human way."
  },
  {
    "objectID": "posts/name_classification/name_classification.html#conclusion",
    "href": "posts/name_classification/name_classification.html#conclusion",
    "title": "Name classification with ChatGPT",
    "section": "Conclusion",
    "text": "Conclusion\nI have explored 4 ways to classify names: from a simple logistic regression to a complex neural network transformer. In the end, a general API from ChatGPT outperformed them all without any proper supervised learning.\n\n\n\nMethod\nAccuracy (%)\n\n\n\n\nBaseline\n89.5\n\n\nBenchmark\n95\n\n\nFastAI\n97.1\n\n\nHugging Face\n96.9\n\n\nChatGPT\n97.5\n\n\n\nThere is a lot of hype around LLMs and ChatGPT, but I’d say it does deserve the attention it’s getting. Those models are transforming tasks that required deep machine learning knowledge into software + prompt engineering problems. As a data scientist, I’m not worried about those models taking over my job, as predictive modelling is only a small aspect of what a data scientist does. For more thoughts on this, check out The Hierarchy of Machine Learning Needs.\n\nAcknowledgements\nI’d like to thank Erich Alves for some of the ideas explored in this post. I also thank Erich and Raphael Tamaki for reviewing the post and giving feedback."
  },
  {
    "objectID": "posts/neurips_2018/index.html",
    "href": "posts/neurips_2018/index.html",
    "title": "NeurIPS 2018: A Data Scientist’s Perspective",
    "section": "",
    "text": "Two weeks ago in Montreal, NeurIPS (formerly known as NIPS) took place, the world’s largest conference on machine learning and artificial intelligence. Major advancements in the field were presented, covering Deep Learning, GANs, and Reinforcement Learning, including both theory and practice. Over eight thousand people attended, more than a thousand papers were accepted, and dozens of workshops were held. Additionally, nearly all major tech companies were present, primarily aiming to recruit scientists and researchers.\nI went to NeurIPS to present the work Adversarial Attacks on Variational Autoencoders at the LatinX in AI workshop, co-authored by George Gondim, myself and our professor Eduardo Valle. At NeurIPS 2017, Prof. Valle presented two works we wrote together: Adversarial Images for Variational Autoencoders at the Adversarial Training workshop and Known Unknowns: Uncertainty Quality in Bayesian Neural Networks at the Bayesian Deep Learning workshop. It’s a great feeling to finally be able to present the work myself!\nSpeaking of LatinX, this year diversity and inclusion were the hottest topics at the conference. We had three other workshops dedicated to this (Black in AI, Women in ML, Queer in AI), as well as smaller talks and gatherings on the subject. Despite this, more than half of the Black in AI participants couldn’t obtain visas in time or were denied, which had some repercussions and may influence future conference locations. At LatinX, the Latin American artificial intelligence meeting Khipu was announced, scheduled to take place in Montevideo in November 2019. We Brazilians cannot miss this opportunity!\nThe conference had its issues: tickets sold out in just 11 minutes; last-minute name change (and we were left without mugs because of it); overcrowded sessions, with people being kicked out due to “fire hazards”; numerous audiovisual problems (it would have been better to watch many talks from home). Despite all of this, the experience is certainly unique: there’s no other place in the world where you might bump into Geoff Hinton, Yoshua Bengio, and Yann LeCun in the halls. Of course, they’ll always be surrounded by fans and people wanting to take photos!\n\n\n\nImagine you’re explaining your poster and Geoff Hinton appears behind you?\n\n\nAlthough these “celebrities” were present, their students were the ones presenting the academic work. With over a thousand papers at the main conference and hundreds at the workshops, it’s hard to know be aware of everything that happened there. I noted down more than 30 articles to review more carefully later. Although this is a large amount to read, it’s only a fraction of the total. For those who want a taste of what’s published there, here are some examples:\nBest papers (according to reviewers and program committee):\n\nNeural Ordinary Differential Equations\nNon-delusional Q-learning and Value-iteration\nOptimal Algorithms for Non-Smooth Distributed Optimization in Networks\nNearly Tight Sample Complexity Bounds for Learning Mixtures of Gaussians via Sample Compression Schemes\n\nBest papers (according to me):\n\nRegularization Learning Networks: Deep Learning for Tabular Datasets\nBayesian Neural Network Ensembles\nThe Everlasting Database: Statistical Validity at a Fair Price\nHow to Start Training: The Effect of Initialization and Architecture\n\nBesides the academic side, NeurIPS has a strong social aspect: parties and gatherings. Excluding the official closing, the parties were all sponsored by companies. I attended two (Nvidia and Element AI), with excellent food and drinks. There’s a bit of elitism: I only managed to attend these two parties thanks to a friend who works with big names in a Montreal lab. However, every day there were several options, with varying degrees of difficulty to get a ticket.\nIn my opinion, the gatherings organized by participants through the conference app were the best part of the week, as they brought together people with similar interests and objectives. I attended AI for Business and AI in Production, both highly relevant to my work as a data scientist at Nubank. At those events, it’s possible to meet people from various backgrounds and profiles who share something with you, without the elitism of those exclusive parties.\n\n\n\nAI for Business lunch (I’m wearing purple in the top right photo)\n\n\nFinally, the last two days were dedicated to workshops. In them, you could delve deep into a specific topic and even learn about things not officially published anywhere yet. The workshop that left the greatest impression on me was AI in Financial Services. However, it had a strong bias toward North American and European realities, mostly discussing regulations and how machine learning could be done within these constraints. For example, in those countries, you need to explain the reasons for denying a loan, but how do you do that if the loan risk is calculated by a deep neural network? However, many countries like China, India, and Brazil don’t have such restrictions, so our challenges are different and were barely explored there. I can say that Nubank is at the forefront of applying machine learning to financial products globally.\nNeurIPS is an academic conference, not the best place to meet other data scientists, but rather the best place to find the top machine learning and AI researchers. I recommend the experience for those with an academic inclination, who have the habit of reading papers and plan to or have already published work in the field. If you have more practical interests, there are conferences and meetings that may be more useful professionally and more accessible, such as Strata, PAPIs.io or even KDD. The most important thing is to get out there and meet new people, but don’t forget to take some time to explore and enjoy the trip!"
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "",
    "text": "This is a true story of how I lost money using machine learning (ML) to bet on Counter-Strike: Global Offensive (CS:GO). The original idea came from a friend, who gave me permission to share it in public. We worked on it together in 2019, but the lessons have stuck with me and I want to share them with the world. Just to make sure I got the numbers right, as this was a long time ago, I retrieved my bets from the betting website to plot my cumulative returns:\nAs you can see, I lost a lot of money very quickly, then my losses plateaued until I grew bored and decided to cut my losses. My friend persisted and he ended up making a 7.5% return on investment (ROI)! We did have an edge after all and I squandered it. This and the next post are a lesson on both how to bet with ML and how to avoid my mistakes. The lessons should apply more broadly than just for e-sports betting, as the concepts to be discussed can be applied to other financial domains like credit, fraud and marketing.\nFirst, on this post, I go over the foundations needed to understand how to make financial decisions with ML:\nSecond, in a future post (follow me on social media to see the second part when it comes out!), I will go over our actual solution:"
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#what-is-your-edge",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#what-is-your-edge",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "What is your edge?",
    "text": "What is your edge?\n\nIf you’re playing a poker game and you look around the table and you can’t tell who the sucker is, it’s you.\n\nIf you want make money by betting or trading, you need to have an edge. While the efficient market hypothesis is a good first approximation and suggests edges shouldn’t exist, clearly there are situations where you can exploit market inefficiencies for monetary gains. That is your edge. That is how funds like Renaissance Technologies make money over decades. For a clear first-person narrative on finding edges and beating casinos and markets, read Ed Thorp’s biography, who proved card counting could be profitable and pioneered quantitative finance. More on him later.\nAccording to Agustin Lebron in the Laws of Trading, an edge is defined as “the set of reasons, the explanation, why you think a given trade has a positive expected value. In other words, why does that trade make money on average?”. More importantly, he highlights that if you can’t explain your edge in 5 minutes, you probably don’t have a very good one.\nWatch out for anecdotal evidence of anyone beating the market, as it could solely be due to survivorship bias. Edges are not sold on the public market, so be skeptical with anyone trying to sell you a way to make money. Whenever an edge becomes public, it’s not an edge anymore.\nThat is why you should read about failure stories, like mine, as they carry important lessons not contaminated by bad incentives or biases. Nassim Taleb, for example, recommends the book What I Learned Losing a Million Dollars as “one of the rare noncharlatanic books in finance”.\nOur edge, or so we thought, was to use the power of tabular ML to beat the bookmakers. We had worked previously with credit, fraud, and insurance, and we believed our skill set in those areas would translate to an advantage here. While that’s a flimsy argument at face value, the backtests we ran indicated we did have an edge indeed (more on that on the next post). We chose CS:GO in particular as it had enough match data available in public that we could scrape and leverage for our models. Plus, we thought it’d be a much more incipient market compared to horse racing or football betting."
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#financial-decision-making-with-ml",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#financial-decision-making-with-ml",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "Financial decision-making with ML",
    "text": "Financial decision-making with ML\nHow can you use ML for sports betting? That has a surprising long history: Decades ago, logistic regression was being used for horse racing betting and made some people rich. Essentially, it’s not different from using ML in any other financial decision-making application, such as giving loans or stopping fraudulent transactions.\n\nLet’s work backwards from a simple profits equation. Profits can be defined as revenue minus costs1 and as a function of actions a. The actions a you take are betting, providing credit, and blocking transactions. You can simply write it as:\n\\[\n\\text{Profits}(a) = \\sum \\text{Revenue}(a) - \\sum \\text{Cost}(a)\n\\]\nWhile simple and naive, defining the revenue and losses for your product or business can be an illuminating exercise. I’d claim that is what differentiates “Kagglers” from money-making data scientists. For more on this topic, read my older post The Hierarchy of Machine Learning Needs. Let’s define action, revenue and cost for the aforementioned examples:\n\nBetting:\n\nAction: Bet size (between 0 and the limit offered by the betting house)\nExpected revenue: Bet size \\(\\cdot\\) Odds \\(\\cdot\\) Probability of winning\nCost: Bet size\n\nLoans\n\nAction: Loan amount (between 0 and some limit imposed by the bank)\nRevenue: Interest rate \\(\\cdot\\) loan amount\nExpected cost: Probability of default \\(\\cdot\\) loan amount\n\nFraud\n\nAction: Blocking the transaction or not (binary)\nRevenue: Transaction amount \\(\\cdot\\) Margins\nExpected cost: Transaction amount \\(\\cdot\\) Probability of fraud\n\n\nML can help by filling in the blanks (in bold): ML models can predict the probability of winning a bet, default of a loan or fraud for a transaction. With such probabilities and the other fixed quantities known (like the interest rate), you can choose an action a that maximizes the profits. Let’s work that out in the case of betting."
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#one-bet-expected-profits-and-decision-rule",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#one-bet-expected-profits-and-decision-rule",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "One bet: Expected profits and decision rule",
    "text": "One bet: Expected profits and decision rule\nSay you have a match between two teams A and B. Assume that the probability of A beating B is P. If you place a bet of \\$10 at odds of 2, you will net \\$20 if A wins and lose \\$10 if A loses. The expected value of the bet on A is:\n\\[\\begin{align}\nE[Profits(bet=10)] &= E[Revenue(bet=10)] - E[Cost(bet=10)] \\\\\n                   &= 2 \\cdot 10 \\cdot P - 10 \\cdot (1 - P) \\\\\n                   &= 30 \\cdot P - 10\n\\end{align}\\]\nThis leads to the following decision rule: if \\(P \\gg 1/3\\), you should bet on A. If \\(P \\ll 1/3\\), you should bet on B. If \\(P \\approx 1/3\\), you should not bet at all2. Also, this implies that, if the market is efficient, then betting odds of 2 implies probability of winning to be 1/3.\nNow, let’s make it broader and consider any betting odds \\(O\\) and that the probability of A beating B is \\(P(X_A, X_B)\\), where \\(X\\) are the features of each team (say, their latest result):\n\\[\\begin{align}\nE[Profits(bet)] &= E[Revenue(bet)] - E[Loss(bet)] \\\\\n                &= O \\cdot bet \\cdot P(X_A, X_B) - bet \\cdot (1 - P(X_A, X_B))\n\\end{align}\\]\nAnd you should bet on A if \\(P(X_A, X_B) \\gg \\frac{1}{O+1}\\) and on B if \\(P(X_A, X_B) \\ll \\frac{1}{O+1}\\), where \\(\\frac{1}{O+1}\\) is the implied probability of winning given by the betting odds. In practice, the implied probability of A beating B and B beating A sum up to more than 1. That is the bookmaker’s margin, which is one major way they make money.\nOne bet is not usually made in isolation, at there are other potential bets you can make in the present and future. The question then becomes not just about betting or not, but rather how much to bet on each match. For simplicity, I will assume bets are made sequentially and on a limited bankroll. How do you maximize the expected value of all your future bets?"
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#multiple-bets-the-kelly-criterion",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#multiple-bets-the-kelly-criterion",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "Multiple bets: The Kelly criterion",
    "text": "Multiple bets: The Kelly criterion\nA typical answer is the Kelly criterion. That is a formula used to determine the optimal size of a series of bets, developed by John Kelly in 1956. It aims to maximize the expected value of the logarithm of your wealth (bankroll)3. I will not bore you with its derivation here, but the end result is a simple formula that provides the fraction of your bankroll to wager for each bet given the odds and the probability of winning:\n\\[\n\\text{Fraction} = \\frac{O \\cdot P(X_A, X_B) - (1 - P(X_A, X_B))}{O}\n\\]\nLet’s go back to the first example and assume \\(P(X_A, X_B)\\) to be 0.5, that is, even probability of A beating B (which makes it a profitable bet in expectation as odds of 2 imply only 1/3 chance of A beating B). The optimal fraction of the bankroll to wager would be:\n\\[\n\\text{Fraction} = \\frac{2 \\cdot 0.5 - 0.5}{2} = 0.25\n\\]\nThat is, you should bet 25% of your bankroll! That sounds suspiciously large, given that you only have a 50% chance of winning.\nUnfortunately, the Kelly criterion is not reliable for real betting/trading as it assumes that the probabilities are known and there is no uncertainty in them, almost never the case4. That makes it way too aggressive and allows for too much variance in the bankroll. The best way to visualize that is by looking at the relationship between the bankroll growth rate and the fraction of bankroll wagered:\n\n\nCode\n# Parameters from the image\np = 0.5\nq = 1.0-p\nb = 2\na = 1\n\n# Growth rate function\ndef growth_rate(f, p, q, b, a):\n    return p * np.log(1 + b*f) + q * np.log(1 - a*f)\n\n# Generate data points for plotting\nf_values = np.linspace(0, 0.75, 500)  # Wagered fraction values from 0% to 50%\nr_values = growth_rate(f_values, p, q, b, a)\n\n# Using plotly express to create the base plot\nfig = px.line(x=f_values, y=r_values, labels={'x': 'Wagered fraction', 'y': 'Growth rate'},\n              title=\"Growth rate as a function of wagered fraction\")\n\n# Formatting axes to show percentages\nfig.update_layout(\n    width=600, \n    height=600,\n    xaxis_tickformat=\",.0%\",  # x-axis percentages\n    yaxis_tickformat=\",.1%\"   # y-axis percentages\n)\n\n# Adding the vertical line\nfig.add_shape(\n    go.layout.Shape(\n        type=\"line\",\n        x0=0.25,\n        y0=0,\n        x1=0.25,\n        y1=0.059,\n        line=dict(color=\"Red\", dash=\"dot\")\n    )\n)\n\n# Annotations\nfig.add_annotation(\n    go.layout.Annotation(\n        text='Optimum \"Kelly\" bet',\n        xref=\"x\",\n        yref=\"y\",\n        x=0.25,\n        y=0.059,\n        showarrow=True,\n        arrowhead=4,\n        ax=60,\n        ay=-40\n    )\n)\n\n# Adding parameters as a footnote\nparam_str = f\"Parameters: P_winning={p}, Odds={b}\"\nfig.add_annotation(\n    go.layout.Annotation(\n        text=param_str,\n        xref=\"paper\",\n        yref=\"paper\",\n        x=0,\n        y=-0.15,\n        showarrow=False,\n        align=\"left\",\n        font=dict(size=10)\n    )\n)\n\nfig.show()\n\n\n\n                                                \n\n\nNote the asymmetry: if you bet too little, you will not grow your bankroll fast enough. But if you bet too much, you will lose your bankroll very quickly. The Kelly criterion is the point where the growth rate is maximized, but the curve is flat around that point, which means that you can bet less and still get a high growth rate without the risk associated with the “optimal betting point”.\nThis is more general than it seems: when I worked in performance marketing, the curve between profits and marketing spend was surprisingly similar. A former colleague writes about it here: Forecasting Customer Lifetime Value - Why Uncertainty Matters.\nHow much should you actually bet, then? A simple heuristic is simply always betting a fixed value, no matter what. That is what I’d suggest when starting and figuring out whether you have an edge and other elements of your strategy. Another suggestion is using the half-Kelly, that is, half of what the Kelly criterion implies. That forms a good compromise between the two extremes, but should only be used when you can trust both your model and betting strategy."
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#probability-calibration",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#probability-calibration",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "Probability calibration",
    "text": "Probability calibration\nSome of the assumptions above are not realistic in real life. The probability \\(P(X_A, X_B)\\) is never exactly known, even if it can be predicted by a ML model trained on historical data (which is the major theme of the second post in this series). But even if you have a good model for that, it doesn’t mean that the probability is calibrated. Also, it assumes the data-generating process to be stationary5, another further complication discussed in the second post.\nBefore we go on further, we need to actually define what probability calibration means: if the model predicts probability of \\(X\\), it should be correct \\(X%\\) of the times. This can be better visualized with a calibration plot, where the dotted blue line represents a perfectly calibrated model and the black line represents the uncalibrated predictions of a fake model:\n\n\nCode\n# Data from the image\nmean_predicted_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nfraction_of_positives = [.05, 0.1, 0.2, .35, 0.5, .65, 0.8, 0.9, 0.95]\n\n# Create a scatter plot for the model's calibration\nfig = go.Figure()\n\n# Add the perfectly calibrated line\nfig.add_trace(go.Scatter(\n    x=[0, 1], y=[0, 1],\n    mode='lines', name='Perfectly calibrated',\n    line=dict(dash='dot')\n))\n\n# Add the model's calibration curve\nfig.add_trace(go.Scatter(\n    x=mean_predicted_values, y=fraction_of_positives,\n    mode='lines+markers', name='Actual model',\n    line=dict(color='black')\n))\n\n# Set layout properties\nfig.update_layout(\n    title=\"Calibration plot\",\n    xaxis_title=\"Mean predicted value\",\n    yaxis_title=\"Fraction of positives\",\n    xaxis=dict(tickvals=[i/10 for i in range(11)], range=[0, 1]),\n    yaxis=dict(tickvals=[i/10 for i in range(11)], range=[0, 1]),\n    showlegend=True\n)\n\nfig.show()\n\n\n\n                                                \n\n\nNote that calibration is not enough: if your model predicts 50% for a coin flip, it’s perfectly calibrated but also totally useless!\nHow do you calibrate a model? If your loss function is a proper scoring rule, like the the negative log-likelihood (NLL), it should come calibrated by default6. But if your loss doesn’t lead to calibration or if empirically you see calibration issues, you can always use a calibration method:\n\nPlatt scaling: logistic regression on the output of the model, usually applied to smooth predictors like MLPs\nIsotonic regression: non-parametric method that fits a piecewise-constant non-decreasing function to the data, usually applied to tree-based models like XGBoost\n\nMeasuring calibration is tricky as there is a trade-off between calibration and discrimination. As the coin flip example shows, a dumb model can be calibrated, while a model with amazing discrimination powers could be uncalibrated. Therefore, you need to to look at both calibration and discrimination (accuracy, AUC, precision-recall, etc) separately. With respect to the calibration part alone, I suggest two assessments:\n\nCalibration plot (like shown above): visual inspection always helps making sense of the data\nBrier score / negative log-likelihood: they are proper scoring rules and they balance calibration and discrimination, so the smaller the better in both senses (0 is perfect but usually not achievable)\n\nIf you want to automate the visual inspection in (1), you can use the expected calibration error (ECE), which is the average difference between the accuracy and the confidence of the predictions. The smaller the ECE, the better the calibration. For more on this topic, read On Calibration of Modern Neural Networks."
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#winners-curse",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#winners-curse",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "Winner’s curse",
    "text": "Winner’s curse\n\nIf it sounds too good to be true, it probably is!\n\nThe winner’s curse is a phenomenon that occurs when buyers overpay for something that they won in an auction, a form of selection bias. Consider the following toy experiment: Say you have N people bidding for a product, the true value of the product is V, and each person estimates the value of the product with \\(v \\sim N(V, \\sigma)\\), where N is a normal distribution and \\(\\sigma\\) is the standard deviation of the estimate. This means that the value estimations are unbiased (correct on average). The person who bids the highest, wins. Assuming a second-price auction7, what is the expected value of the winning bid?\nLet’s assume the item is worth \\$1000, the standard deviation of the estimates is \\$50, and that there are 10 bidders. In a second-price auction, you bet the expected value of the item. If you win, you only have to pay the second highest bid. We can simulate that easily:\n\n\nCode\n# Winner's curse simulation \nV = 1000  # Real value of the item\nN = 10  # Number of bidders\nsigma = 50  # Standard deviation of the bidders' valuations\n\noverpayment = []\nfor _ in range(10_000): # Simulation samples\n    v = np.random.normal(V, sigma, N)\n    winning_bid = np.sort(v)[-2]  # Winning bid pays second highest bid\n    overpayment.append(winning_bid - V)\nmean_overpayment = np.mean(overpayment)\n\n# Create the histogram\nfig = go.Figure()\n\nfig.add_trace(go.Histogram(\n    x=overpayment,\n    name='Overpayment',\n    marker=dict(color='blue'),\n    opacity=0.5,\n))\n\n# Add a vertical line for the mean\nfig.add_shape(\n    go.layout.Shape(\n        type='line',\n        x0=mean_overpayment,\n        x1=mean_overpayment,\n        y0=0,\n        y1=1,\n        yref='paper',\n        line=dict(color='red', width=2, dash='dot')\n    )\n)\n\n# Add annotations and labels\nfig.update_layout(\n    title=\"Winner's Curse: Histogram of Overpayment\",\n    xaxis_title='Overpayment ($)',\n    yaxis_title='Frequency',\n    shapes=[dict(\n        x0=mean_overpayment,\n        x1=mean_overpayment,\n        y0=0,\n        y1=1,\n        yref='paper',\n        line=dict(color='red', width=2, dash='dot')\n    )],\n    annotations=[dict(\n        x=mean_overpayment,\n        y=0.9,\n        yref='paper',\n        showarrow=True,\n        arrowhead=7,\n        ax=0,\n        ay=-40,\n        text=f\"Mean: {mean_overpayment:.2f}\"\n    )]\n)\n\nfig.show()\n\n\n\n                                                \n\n\nThat is, the expected overpayment conditional on winning the auction is \\$50, 5% of the actual value of the item! What is surprising is that the winner’s curse can exist even if there are no biases in the estimates of the item’s value.\nIn auctions, it’s very difficult to be rid of the winner’s curse, as it depends on not just your estimate of the item’s value but also everyone else’s. Plus, it also depends whether it’s a first- or second-price auction. For more, read Richard Thaler’s intro on the topic.\n\nFrom winner’s curse to winning strategy\nFor sports betting, consider the following: If your model prediction differs from the implied betting odds, which one is more likely to be wrong? Also, consider that whenever your model is wildly overconfident, that is when you will bet the most and most wrongly. That is a recipe for disaster, as even a perfectly calibrated and precise model on average could still lead to ruinous betting decisions if you only act when it’s wrong.\nThe only protection against the winner’s curse and selection bias is extensive validation: backtest your strategy and paper-trade before spending real money. The backtest will provide you with a list of bets you would have made in the past and paper-trading will allow you to validate your strategy and collect data in real time.\nWith such “counterfactual” betting data, you can ensure that your model and strategy work not just on expectation but also for the bets you actually would have made. That is, you must evaluate your model metrics, including calibration, on the matches you would have bet on. You can also use this data to calculate the ROI and risk metrics (e.g. Sharpe ratio and maximum drawdown) of your betting strategy in order to further validate your edge and determine its profitability and risk8.\nEven Ed Thorp, when he had a clear and quantified edge by card counting on blackjack, paper-traded and started small until he fully embraced the Kelly criterion. He actually made some mistakes and lost money early on until he figured out the proper winning strategy. Then, he adopted the Kelly criterion and made huge sums until he was banned from and threatened by many casinos. He then moved on to beat the stock market and became a billionaire.\nBacktesting and paper-trading are not enough. You also should have an entry and exist strategy to allow you to validate every piece of your strategy before you go all-in. That is, you should have a plan for when to stop betting and when to increase your bets that is not model-dependent. Simulation is essential but it can only go so far, as there are always unknown unknowns which will only be surfaced when you start betting for real."
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#conclusion",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#conclusion",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, I went over some concepts you need to know to use ML for e-sports betting. They apply more broadly than just CS:GO betting and might be useful for credit, fraud and marketing decisions, as I can attest from my own previous experiences in all of those fields. For example, I developed risk models for credit lines that impacted the bank’s credit exposure by dozens of millions of dollars using a profit-revenue-cost framework that was much more complex but similar in principle to what I describe in this post. I might write about that in the future in more details, so let me know if that interests you.\nI didn’t cover some important topics for financial decision-making which might not be that relevant for betting but might be for other areas, the most glaring omission being causal inference. For more on that, read Causal Inference in Python, written by a former colleague that worked with me in credit.\nFor the next post, to be released soon, I will go over how we actually implemented our solution, including the features and betting odds data, the modelling, evaluation and metrics, and the backtesting showing our strategy had a positive ROI in theory. Also, I comment on why I failed make money given the theoretical edge. Stay tuned!\n\nAcknowledgements\nThis post would not exist without the ideas of Ramon de Oliveira, who worked with me on the project and did most of the implementation. I also thank Raphael Tamaki for his feedback on the post."
  },
  {
    "objectID": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#footnotes",
    "href": "posts/csgo_betting_with_ml_part_1/csgo_betting_with_ml_part_1.html#footnotes",
    "title": "How I lost 1000€ betting on CS:GO — Foundations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor most businesses, what you really want to maximize is the net present value (NPV), which is the sum of cash flows over time, discounted back to their value in present terms. In other words, you care more about profits tomorrow than 10 years from now, but you just don’t want to ignore future profits. For simplicity, we will stick to present-day profits for now.↩︎\nThe reason for using “much greater/lesser” inequalities is due to inefficiencies in the betting process, such as transaction fees, and to the ever-present model prediction errors.↩︎\nThat implies your utility for money is logarithmic, which is a reasonable assumption for most people. That is, if your wealth is \\$100, you care a lot about winning or losing \\$10. But if you are a millionaire, you would only start to worry about winning or losing hundreds of thousands of dollars. While not perfect, it’s a better assumption than linear utility, which would imply you care the same about winning or losing \\$10 regardless of your wealth.↩︎\nThere are some exceptions, like playing blackjack with card counting, where you can have a good estimate of the probability of winning, which was the case when Ed Thorp beat the dealers in Las Vegas in the 60s. More broadly, one should beware the Ludic fallacy, which is the misuse of games to model real-life situations, explained by Taleb as “basing studies of chance on the narrow world of games and dice”.↩︎\nThe data-generating process is the probability distribution \\(P(Y|X)\\) that we are trying to model. In our case, \\(Y\\) is the probability of a team winning given their features \\(X\\). Stationarity means that \\(P(Y|X)\\) does not change over time. This is related but not the same as target shift (\\(Y\\) changes over time) and covariate shift (\\(X\\) changes over time), also possible complications but not discussed further.↩︎\nTrust but verify: even if your model should be calibrated in principle, always make sure to assess its calibration empirically on the test set.↩︎\nFirst-price auctions: the winner pays the amount they bid. Second-price auctions: the winner pays the amount the second-highest bidder bid. Second-price auctions are interesting because the optimal bid is the expected value of the product, so they are adopted in some situations e.g. Meta’s digital ad exchange.↩︎\nRisk and return typically represent a trade-off, where higher returns are associated with higher risks. You should only accept a strategy with high risks if it has high returns, otherwise it’s better to leave your money on treasury bills or index funds. The Sharpe ratio is one metric that captures this trade-off, but it’s not useful in the “extremistan” of fat-tailed returns (e.g. returns of stocks or options). Read the Black Swan before considering investing, trading or betting.↩︎"
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html",
    "href": "posts/hierarchy_needs_ml/index.html",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "",
    "text": "In 1943, Abraham Maslow created the hierarchy of human needs, ranging from basic physiological needs to abstract concepts like self-actualization. In this article, I propose a hierarchy of machine learning needs:\nA framework like this can be useful for answering questions like:\nI try to answer these questions and more at the end of this article, but first, it’s necessary to define and better understand each need in the hierarchy."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#the-hierarchy-of-needs",
    "href": "posts/hierarchy_needs_ml/index.html#the-hierarchy-of-needs",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "The hierarchy of needs",
    "text": "The hierarchy of needs\n\nBusiness\nBusiness sits at the base of the pyramid, as it’s the foundation everything is built upon. Directly or indirectly, data scientists must always strive to deliver value to the business. This way, we can impact customers positively, secure resources (whether human or computational), and advance in our careers. This pragmatic view may be discouraging for those seeking technical challenges only, but I propose that business challenges are more difficult and unique than those found in machine learning competitions.\nNo e-commerce company aims solely to predict customer churn; the real goal is to take actions that reduce churn among the most valuable customers. Credit risk assessment alone isn’t very useful; deciding who will receive credit and for how much is the core of many financial institutions. The probability of lead conversion is just the first step in prioritizing and allocating sales resources in a B2B company. In all three examples, the focus is on intervention (causality) rather than just prediction (machine learning).\nI worked on a document classification project where there initially seemed to be no complicated business issues involved; achieving sufficient accuracy would ensure the project’s success. After talking to the actual users, I realized that the classification rationale was more important than the decision itself. In the same place, a project had previously failed for considering only the model’s AUC without any concern for its practical use. We shifted our focus and delivered a system that was genuinely useful for the users and the company by investing more in the UI and less in the modelling.\n\n\nTarget\nThe target is what the model will try to predict, which may not be obvious at first:\n\nChurn: What should be the time window to define churn? What if the user becomes active again?\nCredit: How long must one go without paying to be a default? What if the collections team gets all the money back?\nSales lead: At what point in the sales funnel do we define conversion? What if there is a refund?\nDocument classification: What should be done with sub-categories? Can we group smaller categories into “others”?\n\nChoosing the target is the most critical step in modelling. No features or models can save an inappropriate target. On the other hand, having an appropriate target allows even very simple models (such as linear regression) with basic features to have some impact and already be deployed to production.\n\n\nEvaluation/Metrics\nEvaluation is the framework that objectively assesses how well a model will perform when deployed. The evaluation process should closely resemble what happens after the model’s deployment. While the standard holdout or cross-validation splits provide a starting point, they are generally insufficient. If the problem evolves over time (as is the case for nearly all business problems), a time-series split should be employed. If the target is censored for six months, there should be a six-month gap between training and testing. If the data contains groups and the model will make predictions for new groups in the future, you should use a group split.\nMetrics judge how well a model is predicting its target. It’s not uncommon to use more than one evaluation metric for the same problem. For example, in a binary classification problem, one might use AUC to assess how well the model ranks examples and log loss to evaluate whether the probabilities are well calibrated. It’s also common to consider business metrics, such as expected conversion rate or the number of credit approvals. These metrics are harder to estimate offline and generally require some assumptions or experiments. Although handling these metrics may be more difficult, they serve as a more powerful guide than traditional machine learning metrics. Plus, communicating results with people from other areas becomes much easier!\n\n\nFeatures\nFeatures are the inputs of the model. They need to be predictive of the target. It’s crucial to avoid leakage, meaning that when deploying the model in production, features must appear in the same way they did during training. I’ve encountered leakage in a conversion prediction project where I used features that only appeared when the user converted. In other cases, the values were missing. The model learned that missing values were never associated with conversions and achieved a 100% AUC. However, this model had no real value for the business!\nFeature engineering can be partially automated with tools like Featuretools. However, most of the work in creating new features depends on understanding the problem and the available data (including what can be crawled or purchased externally).\nAre more features always better? Not necessarily. More features may require more monitoring and engineering, which may not be a good trade-off in certain cases. It’s essential to balance the value of features (possibly with a business metric) with their operational and maintenance costs.\n\n\nModels\nIn the end, given the constraints of business, target, evaluation/metrics, and features, the choice of models narrows considerably. If the business requires interpretability of predictions, you shouldn’t use a neural network. If the target is continuous, you want a regression model, not a classification model. If the metric evaluates the calibration of probabilities, you want a model that can learn a proper scoring rule. If you have more features than examples, you want a model that can ignore most features, like Lasso regression.\nThe modelling process can be automated with tools like auto-sklearn or PyCaret, but only if it makes sense for the business. In some cases, gaining an additional percentage point in accuracy is less useful than having interpretable decisions, communicating with other areas about how the model works (including external regulators), training speed for “big data” cases, and prediction latency for real-time systems."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#applications",
    "href": "posts/hierarchy_needs_ml/index.html#applications",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "Applications",
    "text": "Applications\nWe can apply the hierarchy of needs to better understand the reality of machine learning, which is difficult to learn solely from MOOCs or competitions:\n\nThe Spiral of Applied Machine Learning\nIn practice, machine learning is not a linear process, starting with a business problem and ending with a model. The process is repeated several times, and each iteration feeds the next:\n\nThis suggests starting simple on the modelling side. Always define a baseline first, which can be a business rule of thumb or a simple model (e.g. a linear regression or a decision tree classifier). The first proper model you build to compare against the baseline should be pragmatic, which depends on your domain:\n\nTabular data: Use LightGBM (watch my PyData London presentation here) or XGBoost\nTime series forecasting: Use Auto-ARIMA or Prophet\nText classification: Use word counts and Naive Bayes, TF-IDF and logistic regression, or ChatGPT API\nImage recognition: Use ResNet with transfer learning or ChatGPT4 API\n\nThis will handle the “model” part of the spiral and probably capture most of the gains. Use your time to focus on the other components before coming back to the model: deploy it as soon as possible, get feedback from the stakeholders, explore the failure modes (error analysis), and then go over the target-evaluation-metrics-features-model process again.\n\n\nAuto ML\nMachine learning automation operates only at the highest levels of the pyramid: primarily in modelling, followed by feature engineering and selection. The other needs are much harder to automate and are the biggest differentiators among data scientists since they depend on domain knowledge. Even in the modelling part, there are cases where maximizing a single metric does not capture the whole story:\nOnce, while I was in the process of updating models, an analyst noticed that there was a small sub-population where the new model provided predictions that made no sense. In terms of metrics, there was no doubt that the new model was better, but for the business, it was not good to make mistakes in this sub-population. In this case, since I was using a relatively interpretable model, I managed to discover the reason for the incorrect predictions (related to how the missing values were being handled) and could deploy an appropriate solution.\n\n\nMonitoring\nMonitoring a model in production should be done at all levels of the hierarchy. Monitoring is generally associated with evaluation metrics, but other needs should not be ignored. For example, in a credit card fraud problem, the target is generally censored for several months (since it takes time to determine whether fraud has occurred or not, as it is a manual process), meaning that metrics can only be calculated months after each model decision. In this case, it is important to evaluate how the target is changing over time (using proxies with shorter censoring), monitor whether the model’s output distribution remains stable over time, and whether the distribution of features remains the same, which is a significant challenge in itself. For more information on model monitoring, I highly recommend watching Lina Weichbrodt’s PyData Berlin presentation.\n\n\nKaggle\nIn the case of Kaggle, the entire challenge lies in modelling, feature engineering and evaluation. The metric and target are already given. The business aspect is not explicitly present. With this, we can see the limitations of Kaggle as training and evaluation for a data scientist who will work on real-world problems.\nKaggle is a great tool for its purpose and is highly valued in the selection process for some positions. However, a data scientist needs to go beyond this and try to tackle other types of problems, those that are not well-formulated and therefore are fertile ground for exploring targets, metrics, and the use of predictive models in decision-making."
  },
  {
    "objectID": "posts/hierarchy_needs_ml/index.html#conclusion",
    "href": "posts/hierarchy_needs_ml/index.html#conclusion",
    "title": "The Hierarchy of Machine Learning Needs",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, I tried to convey the perspective of a machine learning practitioner, undoubtedly biased by my own particular experience. However, I believe the points raised are relevant to many other data scientists, especially those coming from an academic background. I hope you can use the hierarchy of needs to better guide decisions made as practitioners or students in the field.\nThis blog post originally appeared in the Data Hackers Medium in 2019. Full disclosure: the translation was done with the help of GPT4. I reviewed the final text and updated some sections, since I’ve personally learned and grown a lot since 2019. I’m open to feedback or suggestions for more applications of the hierarchy of needs framework."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "How I lost 1000€ betting on CS:GO — Foundations\n\n\n\n\n\n\n\n\n\n\nPart 1 of a series of 2 posts where I explore how I lost 1000 euros betting on CS:GO with machine learning (ML). This post covers the foundations of e-sports betting with ML: financial decision-making, expected profits of a bet, multible bets with the Kelly criterion, probability calibration and the winner’s curse.\n\n\n\n\n\n\nJan 4, 2024\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nReal-Time ML Models with Serverless AWS\n\n\nUsing AWS Lambda and API gateway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2023\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nName classification with ChatGPT\n\n\nHow does it compare to machine learning language models?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nThe Hierarchy of Machine Learning Needs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nNeurIPS 2018: A Data Scientist’s Perspective\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2018\n\n\nPedro Tabacof\n\n\n\n\n\n\n  \n\n\n\n\nHow (not) to forecast an election\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2016\n\n\nPedro Tabacof\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Personal blog of Pedro Tabacof.\nCheck out my academic work here.\nReach out to me via Linkedin, Twitter, or last name at gmail dot com."
  }
]