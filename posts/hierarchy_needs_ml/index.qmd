---
title: "The Hierarchy of Needs in Machine Learning"
author: "Pedro Tabacof"
date: "2019-03-15"
categories: []
---

In 1943, Abraham Maslow created the [hierarchy of human needs](https://en.wikipedia.org/wiki/Maslow%27s_hierarchy_of_needs), ranging from basic physiological needs to abstract concepts like self-actualization. In this article, I would like to propose the hierarchy of needs in machine learning.

![](ml_needs_pyramid.png)

A framework like this can be useful for answering questions such as:

1. Will my job as a data scientist be automated in a few years?
2. In terms of personal growth, should I focus on solving more problems on Kaggle or more problems in the real world?
3. What kind of monitoring should I do after deploying a model into production?

I try to answer these and more questions at the end of this article, but first it is necessary to define and better understand each need in the hierarchy.

## The hierarchy of needs

### Business

The business is above everything, or rather, at the base of the pyramid. Directly or indirectly, we should always try to deliver value to the business. This way, we can have an impact on customers, obtain resources, and grow in our careers. This pragmatic view may be discouraging for those who are only seeking technical challenges, but I would like to propose that business challenges are more difficult and unique than those found in machine learning competitions.

No e-commerce company aims solely at predicting customer churn; the goal is actually to take actions that reduce churn among the most valuable customers. Credit risk alone is not useful for much; on the other hand, deciding who will receive credit and how much is the core of many financial institutions. The probability of conversion of each lead is only the first step in prioritizing and allocating sales resources in a B2B company. In the three examples cited above, the interest lies in intervention (causality) and not just prediction (machine learning).

I worked on a document classification project that initially did not seem to have any complicated business issues involved: achieving sufficient accuracy would be enough for the project to be successful. After talking to system users, we realized that the reason for the classification was more important than the decision itself. In the same place, a project had previously failed simply because it only considered the AUC of the model, without any concern for the use of the model in practice. We changed our focus and were able to deliver a system that was actually useful for users and the company.

### Target

The target is what the model will try to predict, which may not be obvious at first:

 * **Churn**: What should be the time window considered? What if the user comes back to life?
 * **Credit**: How to define a default? How long does someone need to be without paying?
 * **Sales lead**: At what point in the sales funnel do we define conversion?
 * **Document classification**: Which categories will be considered? What to do with sub-categories?

Deciding the target is the most important step in modeling. There are no features or models that can save an inappropriate target. On the other hand, having an appropriate target, even very simple models (such as linear regression) with some basic features can already have some impact and be put into production.

### Metrics

Evaluation metrics will indicate how well a model is predicting its target. It is not uncommon to use more than one evaluation metric for the same problem. For example, in a binary classification problem, one can use AUC to evaluate how the model is ranking examples and use logloss to evaluate if the probabilities make sense.

In addition, it is common to take into account business metrics, such as expected conversion rate or number of approvals. These metrics are more difficult to estimate offline, and some assumptions or experiments will generally be necessary. As laborious as it may be to deal with this type of metric, they will be a more powerful guide than traditional machine learning metrics. Additionally, communicating the results with people from other areas becomes easier.

### Features

Features are what go into the model. It is crucial to avoid leakage, meaning that at the moment of putting the model into production, the features must appear in the same way they did during training. I once had a leakage problem when trying to predict conversions, as I used features that only appeared when the user converted. In other cases, the values were missing. The model learned that missing values were never associated with conversion and achieved 100% AUC. However, this model had no value.

Feature engineering can be partially automated with tools like [Featuretools](https://www.featuretools.com/). However, most of the work in creating new features depends on understanding the problem and the available data (including what can be crawled or purchased externally).

Is more always better when it comes to features? Not necessarily, as more features may require more monitoring and engineering, which may not be a good trade-off in certain cases. It is necessary to balance the value of features (perhaps with a business metric) with their operational cost.

### Models

In the end, given the business constraints, target, evaluation metric, and features, the choice of models is considerably reduced. If the business requires interpretability of predictions, you should not use a neural network. If the target is continuous, you want a regression model, not a classification model. If the metric evaluates the calibration of probabilities, you want a model that can learn a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule#Proper_scoring_rules). If you have more features than examples, you want a model that can ignore most of the features, like Lasso regression.

The modeling process can be automated with tools like [auto-sklearn](https://automl.github.io/auto-sklearn/master/) or [PyCaret](https://pycaret.gitbook.io/docs/), but only if it makes sense for the business. In certain occasions, gaining one percentage point of accuracy is not as useful as having interpretability of decisions, communicating to other areas how the model works (including external regulators), training speed in the case of "big data," and prediction latency in real-time systems.

## Applications

We can apply the hierarchy of needs to better understand the reality of machine learning, which is difficult to learn in MOOCs or competitions:

### The spiral of applied machine learning

In practice, machine learning is not done in a linear way, starting with a business problem and ending with a model. The process is repeated several times, and each iteration feeds the next:

![](ml_applied_spiral.png)

### ML automation

Automation of machine learning acts only on some levels of the pyramid: primarily in modeling, then in feature engineering and selection. The other needs are much harder to automate and are the biggest differentiator among data scientists. Even in the modeling part, there are cases where maximizing a metric alone does not capture the whole story:

Once, while I was in the process of updating models, an analyst noticed that there was a small sub-population in which the new model provided predictions that made no sense. In terms of metrics, there was no doubt that the second model was better, but for the business, it was not good to make mistakes in this sub-population. In this case, since I was using a relatively interpretable model, I was able to discover the reason for the wrong predictions (related to how missing values were being handled) and think of an appropriate solution.

### Monitoring

Monitoring a model in production should be done at all levels of the hierarchy. Monitoring is generally associated with evaluation metrics, but other needs should not be ignored. For example, in fraud detection, the target is generally censored for several months since it takes time to determine whether fraud occurred or not, which is a manual process. Therefore, metrics can only be calculated months after each model decision. In this case, it is important to evaluate how the target is changing over time (using proxies for lower censorship), monitor whether the model output distribution remains stable over time, and also whether the distribution of features remains the same, which is a significant challenge in itself.

### Kaggle

In the case of Kaggle, the entire challenge is in modeling and feature engineering. The metric and target are already given, and the business does not appear explicitly. Therefore, we can see the limitations of Kaggle as training and evaluating a data scientist who will work on real problems.

Kaggle is an excellent tool for what it proposes, and it is highly valued in the selection process of some places. However, a data scientist needs to go beyond Kaggle and try to tackle other types of problems, those that are not well-formulated and therefore are a fertile ground for exploring targets, metrics, and using models in decision making.

## Conclusion

In this article, I tried to convey the perspective of a machine learning practitioner, certainly biased by my own particular experience. However, I believe that the points raised are relevant to many other data scientists. I hope that the reader can use the presented hierarchy of needs to better guide their processes and decisions, whether as a practitioner or a student in the field. If anyone has another application of the framework, please do not hesitate to contact me or write a comment.